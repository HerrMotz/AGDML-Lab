{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb773b4b",
   "metadata": {},
   "source": [
    "# Exercise 10- Stacking\n",
    "\n",
    "In this exercise you will implement an ensemble method by learning a stacked regressor.\n",
    "\n",
    "- paul.kahlmeyer@uni-jena.de\n",
    "\n",
    "### Submission\n",
    "- Deadline of submission:\n",
    "        21.06.23 23:59\n",
    "- Submission on [moodle page](https://moodle.uni-jena.de/course/view.php?id=43681)\n",
    "\n",
    "\n",
    "### Help\n",
    "In case you cannot solve a task, you can use the saved values within the `help` directory:\n",
    "- Load arrays with [Numpy](https://numpy.org/doc/stable/reference/generated/numpy.load.html)\n",
    "```\n",
    "np.load('help/array_name.npy')\n",
    "```\n",
    "- Load functions with [Dill](https://dill.readthedocs.io/en/latest/dill.html)\n",
    "```\n",
    "import dill\n",
    "with open('help/some_func.pkl', 'rb') as f:\n",
    "    func = dill.load(f)\n",
    "```\n",
    "\n",
    "to continue working on the other tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d27cc77",
   "metadata": {},
   "source": [
    "# The Dataset\n",
    "\n",
    "We will use a real world dataset used for predicting the [quality of red wine](https://www.kaggle.com/datasets/uciml/red-wine-quality-cortez-et-al-2009).\n",
    "Altough the quality is a discrete value between 0 and 10, we interpret it as a regression task. \n",
    "\n",
    "### Task 1\n",
    "\n",
    "Load the dataset stored in `dataset.csv` and split it into `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1b087f8",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2023-06-21T17:14:20.985873Z",
     "end_time": "2023-06-21T17:14:21.025789Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides   \n0               7.4             0.700         0.00             1.9      0.076  \\\n1               7.8             0.880         0.00             2.6      0.098   \n2               7.8             0.760         0.04             2.3      0.092   \n3              11.2             0.280         0.56             1.9      0.075   \n4               7.4             0.700         0.00             1.9      0.076   \n...             ...               ...          ...             ...        ...   \n1594            6.2             0.600         0.08             2.0      0.090   \n1595            5.9             0.550         0.10             2.2      0.062   \n1596            6.3             0.510         0.13             2.3      0.076   \n1597            5.9             0.645         0.12             2.0      0.075   \n1598            6.0             0.310         0.47             3.6      0.067   \n\n      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates   \n0                    11.0                  34.0  0.99780  3.51       0.56  \\\n1                    25.0                  67.0  0.99680  3.20       0.68   \n2                    15.0                  54.0  0.99700  3.26       0.65   \n3                    17.0                  60.0  0.99800  3.16       0.58   \n4                    11.0                  34.0  0.99780  3.51       0.56   \n...                   ...                   ...      ...   ...        ...   \n1594                 32.0                  44.0  0.99490  3.45       0.58   \n1595                 39.0                  51.0  0.99512  3.52       0.76   \n1596                 29.0                  40.0  0.99574  3.42       0.75   \n1597                 32.0                  44.0  0.99547  3.57       0.71   \n1598                 18.0                  42.0  0.99549  3.39       0.66   \n\n      alcohol  quality  \n0         9.4        5  \n1         9.8        5  \n2         9.8        5  \n3         9.8        6  \n4         9.4        5  \n...       ...      ...  \n1594     10.5        5  \n1595     11.2        6  \n1596     11.0        6  \n1597     10.2        5  \n1598     11.0        6  \n\n[1599 rows x 12 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fixed acidity</th>\n      <th>volatile acidity</th>\n      <th>citric acid</th>\n      <th>residual sugar</th>\n      <th>chlorides</th>\n      <th>free sulfur dioxide</th>\n      <th>total sulfur dioxide</th>\n      <th>density</th>\n      <th>pH</th>\n      <th>sulphates</th>\n      <th>alcohol</th>\n      <th>quality</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7.4</td>\n      <td>0.700</td>\n      <td>0.00</td>\n      <td>1.9</td>\n      <td>0.076</td>\n      <td>11.0</td>\n      <td>34.0</td>\n      <td>0.99780</td>\n      <td>3.51</td>\n      <td>0.56</td>\n      <td>9.4</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7.8</td>\n      <td>0.880</td>\n      <td>0.00</td>\n      <td>2.6</td>\n      <td>0.098</td>\n      <td>25.0</td>\n      <td>67.0</td>\n      <td>0.99680</td>\n      <td>3.20</td>\n      <td>0.68</td>\n      <td>9.8</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7.8</td>\n      <td>0.760</td>\n      <td>0.04</td>\n      <td>2.3</td>\n      <td>0.092</td>\n      <td>15.0</td>\n      <td>54.0</td>\n      <td>0.99700</td>\n      <td>3.26</td>\n      <td>0.65</td>\n      <td>9.8</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11.2</td>\n      <td>0.280</td>\n      <td>0.56</td>\n      <td>1.9</td>\n      <td>0.075</td>\n      <td>17.0</td>\n      <td>60.0</td>\n      <td>0.99800</td>\n      <td>3.16</td>\n      <td>0.58</td>\n      <td>9.8</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7.4</td>\n      <td>0.700</td>\n      <td>0.00</td>\n      <td>1.9</td>\n      <td>0.076</td>\n      <td>11.0</td>\n      <td>34.0</td>\n      <td>0.99780</td>\n      <td>3.51</td>\n      <td>0.56</td>\n      <td>9.4</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1594</th>\n      <td>6.2</td>\n      <td>0.600</td>\n      <td>0.08</td>\n      <td>2.0</td>\n      <td>0.090</td>\n      <td>32.0</td>\n      <td>44.0</td>\n      <td>0.99490</td>\n      <td>3.45</td>\n      <td>0.58</td>\n      <td>10.5</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1595</th>\n      <td>5.9</td>\n      <td>0.550</td>\n      <td>0.10</td>\n      <td>2.2</td>\n      <td>0.062</td>\n      <td>39.0</td>\n      <td>51.0</td>\n      <td>0.99512</td>\n      <td>3.52</td>\n      <td>0.76</td>\n      <td>11.2</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>1596</th>\n      <td>6.3</td>\n      <td>0.510</td>\n      <td>0.13</td>\n      <td>2.3</td>\n      <td>0.076</td>\n      <td>29.0</td>\n      <td>40.0</td>\n      <td>0.99574</td>\n      <td>3.42</td>\n      <td>0.75</td>\n      <td>11.0</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>1597</th>\n      <td>5.9</td>\n      <td>0.645</td>\n      <td>0.12</td>\n      <td>2.0</td>\n      <td>0.075</td>\n      <td>32.0</td>\n      <td>44.0</td>\n      <td>0.99547</td>\n      <td>3.57</td>\n      <td>0.71</td>\n      <td>10.2</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1598</th>\n      <td>6.0</td>\n      <td>0.310</td>\n      <td>0.47</td>\n      <td>3.6</td>\n      <td>0.067</td>\n      <td>18.0</td>\n      <td>42.0</td>\n      <td>0.99549</td>\n      <td>3.39</td>\n      <td>0.66</td>\n      <td>11.0</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n<p>1599 rows Ã— 12 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: load data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import sklearn\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "y = np.array(df['quality'])\n",
    "X = np.array(df.iloc[:,:11])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b222bb8",
   "metadata": {},
   "source": [
    "## $R^2$ Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5683ff45",
   "metadata": {},
   "source": [
    "Sklearn uses the [$R^2$ score](https://en.wikipedia.org/wiki/Coefficient_of_determination) as a quality measure for regressors. Given true values $y$ and predicted values $\\hat{y}$ the $R^2$ score is defined as \n",
    "\n",
    "\\begin{align*}\n",
    "R^2(y, \\hat{y}) &= 1-\\cfrac{\\sum_{i=1}^m(y_i-\\hat{y}_i)^2}{\\sum_{i=1}^m(y_i - \\bar{y})^2}\\,,\n",
    "\\end{align*}\n",
    "where $\\bar{y}$ is the average of $y$.\n",
    "\n",
    "This value is 1 if the predictions match exactly, 0 if we would simply always predict the average and negative if our predictions are worse than this simple baseline.\\\n",
    "In short we aim for a value $>0$ and close to $1$.\n",
    "\n",
    "### Task 2\n",
    "\n",
    "Implement the $R^2$ score.\\\n",
    "Then use scikit learns [Linear Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) model to fit on the dataset and calculate the $R^2$ score.\\\n",
    "Compare your result to the `.score` method of the regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6302e258",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-21T17:14:21.004886Z",
     "end_time": "2023-06-21T17:14:21.056151Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "('R2 score', 0.3605517030386882, '.score', 0.3605517030386882)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def r2_score(X : np.ndarray, y : np.ndarray, y_hat : np.ndarray) -> float:\n",
    "    '''\n",
    "    Calculates coefficient of determination.\n",
    "    \n",
    "    @Params:\n",
    "        X... features // TODO WHY????\n",
    "        y... labels\n",
    "        y_hat.. predictions\n",
    "    \n",
    "    @Returns:\n",
    "        score in (-inf, 1)\n",
    "    '''\n",
    "    \n",
    "    # TODO: implement\n",
    "    return 1 - np.sum((y-y_hat)**2) / np.sum((y-np.mean(y))**2)\n",
    "\n",
    "\n",
    "# TODO: calculate r2 score for linear regressor, compare with .score\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, y)\n",
    "y_hat = lr.predict(X)\n",
    "\n",
    "score0 = r2_score(None, y, y_hat)\n",
    "score1 = lr.score(X, y)\n",
    "\n",
    "\"R2 score\", score0, \".score\", score1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13700b8d",
   "metadata": {},
   "source": [
    "# Stacking\n",
    "\n",
    "The main idea in stacking is to \n",
    "1. learn several heterogenous base models on the original data\n",
    "2. learn a meta model on the predictions of the base models\n",
    "\n",
    "<div>\n",
    "<img src=\"images/stacking.png\" width=\"600\"/>\n",
    "</div>\n",
    "The hope is that the meta model can learn to combine the strengths of the base models (e.g. if model 1 fails, model 3 is strong).\n",
    "Note that in contrast to bagging and boosting the base models must not be of the same method (e.g. decision trees)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bdb752",
   "metadata": {},
   "source": [
    "## Base Models\n",
    "\n",
    "First lets select a set of base models. We can now choose from the wide pool of regression methods.\n",
    "\n",
    "Here we want to use the following models:\n",
    "- [Linear Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "- Polynomial Regression of degree 2 (use a [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html) of [Polynomial Features](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) followed by Linear Regression)\n",
    "- [KNN Regression](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html)\n",
    "- [Decision Tree Regression](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb2f1a5",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "Create a list of base models and evaluate them using crossvalidation (avg. of 10 folds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3c0d0e5",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-21T17:14:21.021787Z",
     "end_time": "2023-06-21T17:14:21.818513Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[0.23554709694307588,\n 0.19022922048363994,\n -0.07634595535227609,\n -0.518416022249292]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: create base models\n",
    "\n",
    "degree = 2\n",
    "alpha = 1e-3\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "linear_regression = LinearRegression()\n",
    "knn_regression = KNeighborsRegressor()\n",
    "dtree_regression = DecisionTreeRegressor()\n",
    "polynomial_regression = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "\n",
    "regressors = [linear_regression, polynomial_regression, knn_regression, dtree_regression]\n",
    "scores = []\n",
    "\n",
    "# TODO: estimate avg. crossvalidation score for each base model\n",
    "for r in regressors:\n",
    "    r.fit(X, y)\n",
    "    scores.append(np.mean(cross_val_score(r, X, y, cv=10)))\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174be917",
   "metadata": {},
   "source": [
    "## Meta Model\n",
    "\n",
    "The meta model uses the predictions of the base models to predict $y$. One can thus view the base models as a feature map for the meta model.\n",
    "\n",
    "In order to train the meta model, **we need the predictions of the base models on unseen data** since this is the scenario we would face at inference time. A simple method is to use **out-of-fold predictions** during training:\n",
    "\n",
    "1. separate the data into k folds.\n",
    "2. hold out one of the folds and train the base models on the other folds.\n",
    "3. predict the held out fold using the base models.\n",
    "4. repeat the above two steps k times to obtain out-of-fold predictions for all k folds.\n",
    "5. feed all the out-of-fold prediction as features (training data) to the meta model.\n",
    "\n",
    "\n",
    "### Task 4\n",
    "\n",
    "Implement the out-of-fold method below.\\\n",
    "Calculate the $R^2$-Score on the out-of-fold predictions for each of the base models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afa6379c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-21T17:14:21.779127Z",
     "end_time": "2023-06-21T17:14:22.064740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LinearRegression(), Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())]), KNeighborsRegressor(), DecisionTreeRegressor()]\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.3323438639409082,\n 0.28079386437036213,\n 0.023571220255133585,\n -0.3356808779528835]"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def oof_prediction(model : sklearn.base.BaseEstimator, X : np.ndarray, y : np.ndarray, k = 5, permutate : bool = False) -> np.ndarray:\n",
    "    '''\n",
    "    Calculates out-of-fold predictions.\n",
    "    \n",
    "    @Params:\n",
    "        model... class with a .fit and .predict method\n",
    "        X... samples\n",
    "        y... labels\n",
    "        \n",
    "    @Returns:\n",
    "        predictions\n",
    "    '''\n",
    "    # TODO: implement\n",
    "    _length = X.shape[0]\n",
    "    _block_size = np.floor(X.shape[0] / k)\n",
    "\n",
    "    kf = KFold(k, shuffle=permutate)\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "        _test_X = X[test_index]\n",
    "        _test_y = y[test_index]\n",
    "\n",
    "        _train_X = X[train_index]\n",
    "        _train_y = y[train_index]\n",
    "\n",
    "        model.fit(_train_X, _train_y)\n",
    "        _y_hat = model.predict(_test_X)\n",
    "\n",
    "        # predictions.append({\n",
    "        #     \"predictions\": _y_hat,\n",
    "        #     \"r2_score\": r2_score(None, _test_y, _y_hat)\n",
    "        # })\n",
    "\n",
    "        predictions.append(_y_hat)\n",
    "\n",
    "    return np.concatenate(predictions)\n",
    "\n",
    "k = 10\n",
    "# TODO: calculate r2 score for oof predictions for each base model\n",
    "oof_scores = []\n",
    "for r in regressors:\n",
    "    # for reproducibility\n",
    "    predictions = oof_prediction(r, X, y, k, permutate=False)\n",
    "    oof_scores.append(r2_score(X, y, predictions))\n",
    "\n",
    "print(regressors)\n",
    "oof_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa1f1a9",
   "metadata": {},
   "source": [
    "Now lets put everything together.\n",
    "\n",
    "### Task 5\n",
    "\n",
    "Implement the following `Stacking` class. Keep in mind the following things:\n",
    "- the meta model is trained on out-of-fold predictions of the base models\n",
    "- the base models are trained on the given dataset\n",
    "- when predicting, we just use the predictions of the base models (no out-of-fold) as input for the meta model\n",
    "\n",
    "Use your class to learn a stacked regressor with **linear regression as meta model** and the base models from Task 3. Evaluate it using crossvalidation (avg. of 10 folds) and compare the score to those of the base models (Task 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "1439"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def smallestDivisor(number: int):\n",
    "    divisor = 5\n",
    "    while number % divisor != 0:\n",
    "        divisor+=1\n",
    "    return divisor\n",
    "\n",
    "smallestDivisor(1439) # because it is f****** prime......."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-21T17:14:22.065745Z",
     "end_time": "2023-06-21T17:14:22.070760Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71c17292",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-21T17:14:22.073760Z",
     "end_time": "2023-06-21T17:14:25.552047Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit base models\n",
      "k=10\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "281096845d9e40d5bbe13c64d09ed283"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit base models\n",
      "k=10\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b803cddb792443b093d86982673ef2c8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c2698484ff2e4418bbf34ada8b370ac9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit base models\n",
      "k=10\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b48c1392ce9746419fc79c698b8e0362"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c10e4647f5354d8f905268a3c28a0d80"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit base models\n",
      "k=10\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d51c05a8886642699b032ac58aa8ffa7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e4eafd221bb349a3803fa02806aa3509"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit base models\n",
      "k=10\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "491cb368700b4097bf6b3d8eeb1a588e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0969e7b72855438d8d6c65e010b29448"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit base models\n",
      "k=10\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5b32f868bb124070b9f63699c959cd3f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d25f41951152408e8df0f324cc72c3ca"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit base models\n",
      "k=10\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d8c954f26c7e4d359ffeb512440b4a7a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "af781f7e9b5649eba574c539e760c1b1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit base models\n",
      "k=10\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5696c5e034414cb79d98b200b600ee1b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e817d88e43ea4b75ad0621a0649fd2cc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit base models\n",
      "k=10\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "77d5726d352647508c6ee89b4859c004"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "51591e1738c8470d9e2144e937c57940"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit base models\n",
      "k=10\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5ff4ab752db44eb1a851e5ad945eb2ca"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f6b2ede49a164aab9cbd4fbe3aaaaf32"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit base models\n",
      "k=10\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ebe57b584dac47c09f955a525f3d58a6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1eb3cd6daaad43d9b98ff8eb1a6f8048"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "0.2470325822724561"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class StackedRegressor(sklearn.base.BaseEstimator):\n",
    "    \n",
    "    def __init__(self, base_models : list, meta_model : sklearn.base.BaseEstimator):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        \n",
    "    def fit(self, X : np.ndarray, y : np.ndarray):\n",
    "        '''\n",
    "        Learns base and meta models.\n",
    "        \n",
    "        @Params:\n",
    "            X... features\n",
    "            y... labels\n",
    "        '''\n",
    "        # TODO: implement\n",
    "        y_hats = []\n",
    "        # print(X.shape[0])\n",
    "        # k = smallestDivisor(X.shape[0])\n",
    "\n",
    "        print(\"Fit base models\")\n",
    "        print(f\"k={k}\")\n",
    "        for m in tqdm(self.base_models):\n",
    "            # make oof predictions\n",
    "            y_hats.append(oof_prediction(m, X, y, k, permutate=False))\n",
    "\n",
    "            # fit base models on the whole dataset\n",
    "            m.fit(X, y)\n",
    "\n",
    "        self.meta_model.fit(np.stack(y_hats, axis=1), y)\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X : np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        Given features, predicts labels.\n",
    "        \n",
    "        @Params:\n",
    "            X... features\n",
    "            \n",
    "        @Returns:\n",
    "            labels as array\n",
    "        '''\n",
    "        # TODO: implement\n",
    "        # feature map with base models\n",
    "        y_hats = []\n",
    "        for m in tqdm(self.base_models):\n",
    "            y_hats.append(m.predict(X))\n",
    "\n",
    "        # predict with metamodel\n",
    "        return self.meta_model.predict(np.stack(y_hats, axis=1))\n",
    "\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        '''\n",
    "        R2-Score, needed for crossvalidation.\n",
    "        \n",
    "        @Params:\n",
    "            X... features\n",
    "            y... labels\n",
    "            \n",
    "        @Returns:\n",
    "            Accuracy when predicting for X.\n",
    "        '''\n",
    "        # TODO: implement\n",
    "        return r2_score(X, y, self.predict(X))\n",
    "\n",
    "# TODO: fit stacked model\n",
    "sr = StackedRegressor(regressors, LinearRegression())\n",
    "sr.fit(X, y)\n",
    "\n",
    "# TODO: evaluate with crossvalidation, compare to base models\n",
    "np.mean(cross_val_score(sr, X, y, cv=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40740021",
   "metadata": {},
   "source": [
    "### Task 6\n",
    "\n",
    "Use the [scikit-learn implementation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingRegressor.html) to learn a stacked regressor.\\\n",
    "Evaluate it using crossvalidation (avg. of 10 folds) and compare the score to the scores of task 5.\n",
    "\n",
    "Note that minor differences can occur due to a more advanced oof-prediction used in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f00c97f1",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-06-21T15:03:07.066682087Z",
     "start_time": "2023-06-21T15:02:56.579166544Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/63 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "52ef678220ca464ca0ef89b0df7ec720"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Models: ['Linear Regression']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Polynomial Regression']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['KNN Regression']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Paul ist cool ðŸ˜Ž']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Kernel Ridge']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Support Vector']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Linear Regression', 'Polynomial Regression']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Linear Regression', 'KNN Regression']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Linear Regression', 'Paul ist cool ðŸ˜Ž']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Linear Regression', 'Kernel Ridge']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Linear Regression', 'Support Vector']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Polynomial Regression', 'KNN Regression']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Polynomial Regression', 'Paul ist cool ðŸ˜Ž']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Polynomial Regression', 'Kernel Ridge']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Polynomial Regression', 'Support Vector']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['KNN Regression', 'Paul ist cool ðŸ˜Ž']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['KNN Regression', 'Kernel Ridge']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['KNN Regression', 'Support Vector']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Paul ist cool ðŸ˜Ž', 'Kernel Ridge']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Paul ist cool ðŸ˜Ž', 'Support Vector']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Kernel Ridge', 'Support Vector']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Linear Regression', 'Polynomial Regression', 'KNN Regression']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Linear Regression', 'Polynomial Regression', 'Paul ist cool ðŸ˜Ž']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Linear Regression', 'Polynomial Regression', 'Kernel Ridge']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Linear Regression', 'Polynomial Regression', 'Support Vector']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Linear Regression', 'KNN Regression', 'Paul ist cool ðŸ˜Ž']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Linear Regression', 'KNN Regression', 'Kernel Ridge']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Linear Regression', 'KNN Regression', 'Support Vector']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Linear Regression', 'Paul ist cool ðŸ˜Ž', 'Kernel Ridge']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Linear Regression', 'Paul ist cool ðŸ˜Ž', 'Support Vector']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Linear Regression', 'Kernel Ridge', 'Support Vector']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Polynomial Regression', 'KNN Regression', 'Paul ist cool ðŸ˜Ž']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Polynomial Regression', 'KNN Regression', 'Kernel Ridge']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Polynomial Regression', 'KNN Regression', 'Support Vector']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Polynomial Regression', 'Paul ist cool ðŸ˜Ž', 'Kernel Ridge']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Polynomial Regression', 'Paul ist cool ðŸ˜Ž', 'Support Vector']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Polynomial Regression', 'Kernel Ridge', 'Support Vector']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['KNN Regression', 'Paul ist cool ðŸ˜Ž', 'Kernel Ridge']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['KNN Regression', 'Paul ist cool ðŸ˜Ž', 'Support Vector']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['KNN Regression', 'Kernel Ridge', 'Support Vector']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Paul ist cool ðŸ˜Ž', 'Kernel Ridge', 'Support Vector']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Linear Regression', 'Polynomial Regression', 'KNN Regression', 'Paul ist cool ðŸ˜Ž']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Linear Regression', 'Polynomial Regression', 'KNN Regression', 'Kernel Ridge']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Linear Regression', 'Polynomial Regression', 'KNN Regression', 'Support Vector']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Linear Regression', 'Polynomial Regression', 'Paul ist cool ðŸ˜Ž', 'Kernel Ridge']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Linear Regression', 'Polynomial Regression', 'Paul ist cool ðŸ˜Ž', 'Support Vector']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Linear Regression', 'Polynomial Regression', 'Kernel Ridge', 'Support Vector']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Linear Regression', 'KNN Regression', 'Paul ist cool ðŸ˜Ž', 'Kernel Ridge']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Linear Regression', 'KNN Regression', 'Paul ist cool ðŸ˜Ž', 'Support Vector']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Linear Regression', 'KNN Regression', 'Kernel Ridge', 'Support Vector']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Linear Regression', 'Paul ist cool ðŸ˜Ž', 'Kernel Ridge', 'Support Vector']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Polynomial Regression', 'KNN Regression', 'Paul ist cool ðŸ˜Ž', 'Kernel Ridge']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Polynomial Regression', 'KNN Regression', 'Paul ist cool ðŸ˜Ž', 'Support Vector']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Polynomial Regression', 'KNN Regression', 'Kernel Ridge', 'Support Vector']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Polynomial Regression', 'Paul ist cool ðŸ˜Ž', 'Kernel Ridge', 'Support Vector']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['KNN Regression', 'Paul ist cool ðŸ˜Ž', 'Kernel Ridge', 'Support Vector']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Linear Regression', 'Polynomial Regression', 'KNN Regression', 'Paul ist cool ðŸ˜Ž', 'Kernel Ridge']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Linear Regression', 'Polynomial Regression', 'KNN Regression', 'Paul ist cool ðŸ˜Ž', 'Support Vector']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Linear Regression', 'Polynomial Regression', 'KNN Regression', 'Kernel Ridge', 'Support Vector']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Linear Regression', 'Polynomial Regression', 'Paul ist cool ðŸ˜Ž', 'Kernel Ridge', 'Support Vector']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Linear Regression', 'KNN Regression', 'Paul ist cool ðŸ˜Ž', 'Kernel Ridge', 'Support Vector']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Polynomial Regression', 'KNN Regression', 'Paul ist cool ðŸ˜Ž', 'Kernel Ridge', 'Support Vector']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n",
      "Base Models: ['Linear Regression', 'Polynomial Regression', 'KNN Regression', 'Paul ist cool ðŸ˜Ž', 'Kernel Ridge', 'Support Vector']\n",
      "Final: LinearRegression()\n",
      "Final: Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Final: KNeighborsRegressor()\n",
      "Final: DecisionTreeRegressor()\n",
      "Final: KernelRidge()\n",
      "Final: SVR()\n"
     ]
    }
   ],
   "source": [
    "# TODO: fit sklearn stacked model\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from itertools import chain, combinations\n",
    "\n",
    "from sklearn.linear_model import QuantileRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn import svm\n",
    "\n",
    "def powerset(iterable):\n",
    "    s = list(iterable)\n",
    "    return list(chain.from_iterable(combinations(s, r) for r in range(1,len(s)+1)))\n",
    "\n",
    "results = []\n",
    "\n",
    "def stacking_regressor_helper(estimators):\n",
    "    for set in tqdm(powerset(estimators)):\n",
    "        set = [x for x in set]\n",
    "        # print(type(set), set)\n",
    "\n",
    "        print(\"Base Models:\", [a[0] for a in set])\n",
    "\n",
    "        for e in estimators:\n",
    "            print(\"Final:\", e[1])\n",
    "            reg = StackingRegressor(\n",
    "                estimators=set,\n",
    "                final_estimator=e[1]\n",
    "            )\n",
    "            reg.fit(X, y)\n",
    "\n",
    "            # TODO: evaluate with crossvalidation, compare to custom model\n",
    "            # print(\", \".join([a[0] for a in set]))\n",
    "            results.append(\n",
    "                (e, [a[0] for a in set], np.mean(cross_val_score(reg, X, y, cv=10)))\n",
    "            )\n",
    "\n",
    "estimators = [\n",
    "    ('Linear Regression', linear_regression),\n",
    "    ('Polynomial Regression', polynomial_regression),\n",
    "    ('KNN Regression', knn_regression),\n",
    "    ('Paul ist cool ðŸ˜Ž', dtree_regression),\n",
    "    ('Kernel Ridge', KernelRidge()),\n",
    "    ('Support Vector', svm.SVR())\n",
    "]\n",
    "stacking_regressor_helper(estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6550fa29",
   "metadata": {},
   "source": [
    "### Task 7\n",
    "Try at least two different combinations of regressors for base models and meta model and report the average crossvalidation score.\n",
    "[Here](https://scikit-learn.org/stable/supervised_learning.html) you can find an overview page of sklearn estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7839301f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-21T14:51:11.661998671Z",
     "start_time": "2023-06-21T14:51:11.649137259Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: try different combinations\n",
    "\n",
    "# see task 6\n",
    "pd.DataFrame(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e1315e6714f2518a6216a6eec3b047587d10875bf19b853b35d3e5c84c569e2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
