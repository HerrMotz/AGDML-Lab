{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8081b1b1",
   "metadata": {},
   "source": [
    "# Exercise 7 - Bagging and Boosting\n",
    "\n",
    "In this exercise we will learn how to use an ensemble of weak learners to create a strong learner.\n",
    "Bagging and Boosting will combine weak learners high variance and high bias respectively to a strong learner. \n",
    "\n",
    "In the event of a persistent problem, do not hesitate to contact the course instructor under\n",
    "\n",
    "- paul.kahlmeyer@uni-jena.de\n",
    "\n",
    "### Submission\n",
    "- Deadline of submission:\n",
    "        31.05.23 23:59\n",
    "- Submission on [moodle page](https://moodle.uni-jena.de/course/view.php?id=43681)\n",
    "\n",
    "\n",
    "### Help\n",
    "In case you cannot solve a task, you can use the saved values within the `help` directory:\n",
    "- Load arrays with [Numpy](https://numpy.org/doc/stable/reference/generated/numpy.load.html)\n",
    "```\n",
    "np.load('help/array_name.npy')\n",
    "```\n",
    "- Load functions with [Dill](https://dill.readthedocs.io/en/latest/dill.html)\n",
    "```\n",
    "import dill\n",
    "with open('help/some_func.pkl', 'rb') as f:\n",
    "    func = dill.load(f)\n",
    "```\n",
    "\n",
    "to continue working on the other tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205f529b",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Similar to last exercise, we will use a synthetic dataset for binary classification created with scikit learn (see [here](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d504a9e",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "Load and display the data located in `X.npy` and `y.npy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df859888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# TODO: load and display data\n",
    "import numpy as np\n",
    "data_X = np.load(\"X.npy\")\n",
    "data_y = np.load(\"y.npy\")\n",
    "\n",
    "plt.scatter(data_X[:,0], data_X[:,1], c=data_y, cmap='bwr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ea9d3c",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "We will use decision trees as our model, since Bagging and Boosting have first been developed for these models. Note however that we could in principle replace the decision tree with any other classificator.\n",
    "\n",
    "### Task 2\n",
    "\n",
    "Use scikit learn to learn a decision tree on the dataset, calculate the accuracy and display the decision boundary (you can use `utils.py`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f0e05f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: learn decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import utils\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=2).fit(data_X, data_y)\n",
    "# + print accuracy\n",
    "accuracy = len(np.where(data_y == dt.predict(data_X))[0]) / len(data_y)\n",
    "score = dt.score(data_X, data_y)\n",
    "\n",
    "# + plot decision boundary\n",
    "utils.plot_dec_boundary(data_X, data_y, dt)\n",
    "\"accuracy\", accuracy * 100, \"%\", \"score\", score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c994ae1",
   "metadata": {},
   "source": [
    "# Crossvalidation\n",
    "\n",
    "Until now, we used a train-test split of our data to evaluate how well a model generalizes on unseen data.\n",
    "The test error is however greatly affected by the split. We could have a really bad split, where the test data is fundamentally different from the training data.\n",
    "\n",
    "To overcome this issue, one typically uses **Crossvalidation** - a process where we average the performance of a model on the test data of multiple splits.\n",
    "\n",
    "A commonly used procedure is called [**k-fold Crossvalidation**](https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation):\n",
    "\n",
    "1. Shuffle the dataset\n",
    "2. Split the dataset into $k$ equal sized parts\n",
    "3. For $i = 1,\\dots,k$:\\\n",
    "    i) Use part $i$ as testdata, the other parts as trainingdata\\\n",
    "    ii) Train model on traindata\\\n",
    "    iii) Test model on testdata, save test score\n",
    "4. Report all $k$ test scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60052f96",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "Implement the following cross validation function and evaluate the average crossvalidation score for the decision tree from Task 2 with $k = 10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac8a756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cv(X : np.ndarray, y : np.ndarray, estimator, k : int, seed : int = 0) -> np.ndarray:\n",
    "    '''\n",
    "    Performs k-fold crossvalidation of an estimator.\n",
    "    \n",
    "    @Params:\n",
    "        X... features\n",
    "        y... labels\n",
    "        estimator... object that has .fit(X,y), .predict(X) and .score(X,y) methods\n",
    "        k... number of folds\n",
    "        seed... for reproducibility\n",
    "    \n",
    "    @Returns:\n",
    "        array with test scores for each of the k splits\n",
    "    '''\n",
    "    _length = X.shape[0]\n",
    "    _block_size = np.floor(X.shape[0] / k)\n",
    "\n",
    "    # _X = copy.deepcopy(X)\n",
    "    # _y = copy.deepcopy(y)\n",
    "    #\n",
    "    # np.random.seed(seed)\n",
    "    # np.random.shuffle(_X)\n",
    "    #\n",
    "    # np.random.seed(seed)\n",
    "    # np.random.shuffle(_y)\n",
    "\n",
    "    permutation = np.random.permutation(_length)\n",
    "    _X = X[permutation]\n",
    "    _y = y[permutation]\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for i in range(k):\n",
    "        _mask = np.array([True if i*_block_size <= j < (i+1)*_block_size else False for j in range(_length)])\n",
    "\n",
    "        _test_X = _X[_mask]\n",
    "        _test_y = _y[_mask]\n",
    "\n",
    "        _train_X = _X[~_mask]\n",
    "        _train_y = _y[~_mask]\n",
    "\n",
    "        estimator.fit(_train_X, _train_y)\n",
    "        scores.append(estimator.score(_train_X, _train_y))\n",
    "\n",
    "    return scores\n",
    "\n",
    "# TODO: calculate avg. CV score for k = 10\n",
    "dt2 = DecisionTreeClassifier(max_depth=2)\n",
    "scores = k_fold_cv(data_X, data_y, DecisionTreeClassifier(max_depth=2), 10)\n",
    "\"avg. CV score\", np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a2c476",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "\n",
    "In the following we want to use scikit learn to perform crossvalidation.\n",
    "\n",
    "Use scikit learns [`cross_val_score`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) function to perform a 10-fold crossvalidation, similar to Task 3.\n",
    "Again report the average crossvalidation score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0bb785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: calculate crossvalidation scores with sklearn\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores2 = cross_val_score(DecisionTreeClassifier(max_depth=2), data_X, data_y, cv=10)\n",
    "\"avg. score\", np.mean(scores2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c8bd65",
   "metadata": {},
   "source": [
    "# Bagging - Variance Reduction\n",
    "\n",
    "In Bagging we have learners with **high variance**. \n",
    "\n",
    "<div>\n",
    "<img src=\"images/bagging.png\" width=\"700\"/>\n",
    "</div>\n",
    "\n",
    "The idea is now to learn multiple of these weak learners **in parallel** and use an aggregated decision from all learners for prediction. The hope is that this way the ensemble can overthrow the overfit of a single learner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8baf6e3",
   "metadata": {},
   "source": [
    "## Bootstrapping\n",
    "\n",
    "Since we would get the same learner each time if we learn on the same data, each learner is trained on a **bootstrap** dataset.\n",
    "\n",
    "The idea is simple: Draw samples at random from the original dataset.\n",
    "\n",
    "### Task 5\n",
    "\n",
    "Implement the bootstrap function. Use it to sample a bootstrap dataset and display the densities of the original and bootstrap data. You can use `utils.plot_density` or create your own visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db61bafa",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T18:47:40.969391146Z",
     "start_time": "2023-05-31T18:47:40.817889695Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msample_bootstrap\u001B[39m(X : \u001B[43mnp\u001B[49m\u001B[38;5;241m.\u001B[39mndarray, y : np\u001B[38;5;241m.\u001B[39mndarray, n_samples : \u001B[38;5;28mint\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mtuple\u001B[39m:\n\u001B[1;32m      2\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;03m    Creates a bootstrap dataset.\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124;03m    \u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;124;03m        X, y of bootstrap dataset\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;124;03m    '''\u001B[39;00m\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;66;03m# TODO: implement\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "def sample_bootstrap(X : np.ndarray, y : np.ndarray, n_samples : int) -> tuple:\n",
    "    '''\n",
    "    Creates a bootstrap dataset.\n",
    "    \n",
    "    @Params:\n",
    "        X... features\n",
    "        y... labels\n",
    "        n_samples... number of samples in bootstrap dataset\n",
    "        \n",
    "    @Returns:\n",
    "        X, y of bootstrap dataset\n",
    "    '''\n",
    "    # TODO: implement\n",
    "    _length = X.shape[0]\n",
    "    permutation = np.random.permutation(_length)\n",
    "    _X = X[permutation]\n",
    "    _y = y[permutation]\n",
    "    _block_size = np.floor(X.shape[0] / n_samples)\n",
    "\n",
    "    bootstrap = []\n",
    "    for i in range(n_samples):\n",
    "        _mask = np.array([True if i*_block_size <= j < (i+1)*_block_size else False for j in range(_length)])\n",
    "        bootstrap.append((_X[_mask], y[_mask]))\n",
    "\n",
    "    return bootstrap\n",
    "\n",
    "\n",
    "bs = sample_bootstrap(data_X, data_y, N+1)\n",
    "bs.insert(0, (data_X, data_y))\n",
    "N = len(bs)\n",
    "fig, ax = plt.subplots(N, 1, figsize=(8,50))\n",
    "# TODO: draw bootstrap dataset\n",
    "# TODO: visualize distribution of bootstrap and original data\n",
    "for i, _bs in enumerate(bs):\n",
    "    _X = _bs[0]\n",
    "    _y = _bs[1]\n",
    "    # print(_X, _y)\n",
    "    utils.plot_density(_X, np.max(_X, axis=0), np.min(_X, axis=0), ax[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1e8565",
   "metadata": {},
   "source": [
    "## Bootstrap Aggregation\n",
    "\n",
    "Bagging now involves the two steps:\n",
    "\n",
    "1. **Training**: Train $k$ learners on bootstrapped datasets (with the same size as the original dataset).\n",
    "2. **Prediction**: Output the class label that most of the $k$ learners predict (majority vote).\n",
    "\n",
    "### Task 6\n",
    "\n",
    "Implement the Bagging class for decision trees. \n",
    "\n",
    "Use scikit learn to compute the crossvalidation scores ($k = 10$) for Bagging with 10 decision trees and maximum depth of 10.\n",
    "\n",
    "\n",
    "**Note**:\n",
    "- remember to empty the tree list for each fit. Otherwise crossvalidation will always append to this list.\n",
    "- in order to use your own class in `cross_val_score` just inherit from `sklearn.base.BaseEstimator` and implement `fit` and `score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eb7318",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class DecisionTreeBagging(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, n_trees : int, max_depth : int):\n",
    "        '''\n",
    "        @Params:\n",
    "            n_trees... number of decision trees in ensemble\n",
    "            max_depth... maximum_depth of decision trees\n",
    "        '''\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        \n",
    "    def fit(self, X : np.ndarray, y : np.ndarray):\n",
    "        '''\n",
    "        Learns ensemble of decision trees with Bagging.\n",
    "        \n",
    "        @Params:\n",
    "            X... features\n",
    "            y... labels\n",
    "        '''\n",
    "        # TODO: implement\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X : np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        Given features, predicts labels.\n",
    "        \n",
    "        @Params:\n",
    "            X... features\n",
    "            \n",
    "        @Returns:\n",
    "            labels as array\n",
    "        '''\n",
    "        # TODO: implement\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def score(self, X, y):\n",
    "        '''\n",
    "        Accuracy. \n",
    "        Needed for crossvalidation.\n",
    "        \n",
    "        @Params:\n",
    "            X... features\n",
    "            y... labels\n",
    "            \n",
    "        @Returns:\n",
    "            Accuracy when predicting for X.\n",
    "        '''\n",
    "        # TODO: implement\n",
    "        pass\n",
    "    \n",
    "# TODO: crossvalidation 10 trees, max_depth 10, k = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72dd812",
   "metadata": {},
   "source": [
    "### Task 7\n",
    "\n",
    "For an increasing number of trees in the ensemble, track the average crossvalidation score over 10 tries.\n",
    "\n",
    "Again, use $k = 10$ folds and a maximum depth of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430d9269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: increase number of trees, track avg. CV score over 10 tries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b4b3f1",
   "metadata": {},
   "source": [
    "# Boosting - Bias Reduction\n",
    "\n",
    "In Boosting we have learners with **high bias**. \n",
    "\n",
    "<div>\n",
    "<img src=\"images/boosting.png\" width=\"700\"/>\n",
    "</div>\n",
    "\n",
    "The idea is here to learn multiple of these weak learners **in sequence** and use an aggregated decision from all learners for prediction. Each learner learns on data where the previous learner failed. The hope is that this way we can get finer decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8574dba",
   "metadata": {},
   "source": [
    "### Task 8\n",
    "\n",
    "Use scikit learn to learn a decision tree with high bias on the data. Again display the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977bcd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: learn decision tree with high bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25c9b04",
   "metadata": {},
   "source": [
    "## Weighted Samples\n",
    "\n",
    "Each learner $L_{k}$ assigns different importance to samples depending on its predecessor learner $L_{k-1}$. This importance is realized by assigning a weight to each sample.\n",
    "\n",
    "Let $L$ be a learner and $w$ be the vector of weights for each sample.\n",
    "\n",
    "Then we define the weighted accuracy as \n",
    "\\begin{align}\n",
    "\\text{Acc}_w = \\cfrac{\\sum_{i=1}^m w_i\\cdot \\mathbb{1}[L(x_i)=y_i]}{\\sum_{i=1}^m wi}\\,,\n",
    "\\end{align}\n",
    "where $\\mathbb{1}$ is the indicator function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe6994a",
   "metadata": {},
   "source": [
    "### Task 9\n",
    "\n",
    "Implement the following function and calculate the weighted accuracy for the decision tree from task 8 if we assume equal weights for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395369c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_acc(X : np.ndarray, y : np.ndarray, estimator, weights : np.ndarray) -> float:\n",
    "    '''\n",
    "    Calculates weighted accuracy.\n",
    "    \n",
    "    @Params:\n",
    "        X... features\n",
    "        y... labels\n",
    "        estimator... object with .predict function\n",
    "        weights... weights for each x in X\n",
    "    \n",
    "    @Returns:\n",
    "        weighted accuracy\n",
    "    '''\n",
    "    # TODO: implement\n",
    "    pass\n",
    "\n",
    "\n",
    "# TODO: calculate weighted accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfc7e4e",
   "metadata": {},
   "source": [
    "## Importance of say\n",
    "\n",
    "Learners with high accuracy should have more say in the final decision.\n",
    "\n",
    "The so called **importance of say** is defined as:\n",
    "\\begin{align*}\n",
    "\\alpha = \\log\\left(\\cfrac{\\text{Acc}_w}{1 - \\text{Acc}_w}\\right)\\,.\n",
    "\\end{align*}\n",
    "\n",
    "### Task 10\n",
    "\n",
    "Implement the following function and calculate the importance of say of the decision tree on the dataset with equal weights.\n",
    "\n",
    "Make sure the function does not fail in the extreme cases $\\text{Acc}_w = 0$ and $\\text{Acc}_w = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f1ba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imp_of_say(X, y, estimator, weights):\n",
    "    '''\n",
    "    Calculates weighted accuracy.\n",
    "    \n",
    "    @Params:\n",
    "        X... features\n",
    "        y... labels\n",
    "        estimator... object with .predict function\n",
    "        weights... weights for each x in X\n",
    "    \n",
    "    @Returns:\n",
    "        weighted accuracy\n",
    "    '''\n",
    "    # TODO: implement\n",
    "    pass\n",
    "\n",
    "# TODO: calculate importance of say"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5459cbc",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "With the importance of say the ensemble of learners $L_1,\\dots, L_k$ predicts a label based on the sign of the individual predictions (transformed into -1, 1 labels) weighted with the importance of say $\\alpha$: \n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y}_i = \\mathbb{1}\\left[\\left(\\sum_k \\alpha_k\\cdot (2L_k(x_i) - 1)\\right) > 0\\right]\n",
    "\\end{align}\n",
    "\n",
    "Note how an extremely low accuracy (worse than random with 0.5) will lead to a negative importance of say which will in turn invert the sign of the predicton. \n",
    "\n",
    "### Task 11\n",
    "\n",
    "Use the calculated importance of say $\\alpha$ from task 10 and predict the labels for the dataset.\n",
    "\n",
    "Calculate the accuracy on these predictions and compare it to the accuracy of the decision tree (`.score`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a59c01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: predict + compare accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a0c2dd",
   "metadata": {},
   "source": [
    "## Adjust weights\n",
    "\n",
    "After a learner is done learning, we adjust the sample weights for the next learner. Roughly speaking, we want to increase the weights of misclassified samples, so the next learner only performs good if these critical samples are done right.\n",
    "\n",
    "Let $L$ be the current learner with importance of say $\\alpha$ and $w$ be the vector of weights. Then we update each entry $w_i$ with:\n",
    "\n",
    "\\begin{align}\n",
    "w_i \\rightarrow w_i\\cdot\\exp\\left(\\alpha\\cdot\\mathbb{1}[L(x_i) \\neq y_i]\\right)\\,,\n",
    "\\end{align}\n",
    "where $\\mathbb{1}$ is the indicator function.\n",
    "\n",
    "Note how:\n",
    "\n",
    "- If we have a very good classifier, we have a large $\\alpha$ and we increase the weight of misclassified samples drastically (so they finally get classified correctly with the next classifier)\n",
    "- If we have a very bad classifier with large negative $\\alpha$, we actually have a good classifier that just flips the labels wrong (systematic failure). We account for this with our prediction and can consider these samples well covered by this classifier. Hence we decrease the weight of these samples\n",
    "\n",
    "The next learner now uses these weights to focus the training on samples with high weight.\n",
    "This is done by learning on a bootstrap dataset which is draw **according to the weights**. Samples with high weight are drawn more often and hence the learner is more likely to focus on these samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbe63a0",
   "metadata": {},
   "source": [
    "### Task 12\n",
    "\n",
    "Implement the weighted bootstrap sampling function.\n",
    "\n",
    "Calculate the new weights and draw bootstrap samples according to the old, weights and the new, adjusted weights.\n",
    "\n",
    "Similar to task 5, display the densities of the original and bootstrapped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48c6c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_sample_bootstrap(X : np.ndarray, y : np.ndarray, n_samples : int, weights : np.ndarray) -> tuple:\n",
    "    '''\n",
    "    Creates a weighted bootstrap dataset.\n",
    "    \n",
    "    @Params:\n",
    "        X... features\n",
    "        y... labels\n",
    "        n_samples... number of samples in bootstrap dataset\n",
    "        weights... weights for individual samples\n",
    "    @Returns:\n",
    "        X, y of bootstrap dataset\n",
    "    '''\n",
    "    # TODO: implement\n",
    "    pass\n",
    "\n",
    "# TODO: calculate new weights + sample bootstrap data\n",
    "\n",
    "# TODO: plot densities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f047d6",
   "metadata": {},
   "source": [
    "## Build boosted Ensemble \n",
    "\n",
    "Now we have everything to build a Boosted decision tree ensemble.\n",
    "Starting with equal weights, we perform the following steps for each learner:\n",
    "\n",
    "1. Bootstrap Dataset according to weights\n",
    "2. Learn a weak learner\n",
    "3. Score Performance $\\rightarrow$ weight of learner\n",
    "4. Adjust weights of samples\n",
    "\n",
    "\n",
    "### Task 13\n",
    "\n",
    "Implement the Boosting class for decision trees. \n",
    "\n",
    "Use scikit learn to compute the crossvalidation scores ($k = 10$) for Boosting with 10 decision trees and maximum depth of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33caf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class DecisionTreeBoosting(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, n_trees : int, max_depth : int):\n",
    "        '''\n",
    "        @Params:\n",
    "            n_trees... number of decision trees in ensemble\n",
    "            max_depth... maximum_depth of decision trees\n",
    "        '''\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        self.alphas = []\n",
    "        \n",
    "    def fit(self, X : np.ndarray, y : np.ndarray):\n",
    "        '''\n",
    "        Learns ensemble of decision trees with Bagging.\n",
    "        \n",
    "        @Params:\n",
    "            X... features\n",
    "            y... labels\n",
    "        '''\n",
    "        # TODO: implement\n",
    "        pass\n",
    "        \n",
    "        \n",
    "    def predict(self, X : np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        Given features, predicts labels.\n",
    "        \n",
    "        @Params:\n",
    "            X... features\n",
    "            \n",
    "        @Returns:\n",
    "            labels as array\n",
    "        '''\n",
    "        # TODO: implement\n",
    "        pass\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        '''\n",
    "        Accuracy. \n",
    "        Needed for crossvalidation.\n",
    "        \n",
    "        @Params:\n",
    "            X... features\n",
    "            y... labels\n",
    "            \n",
    "        @Returns:\n",
    "            Accuracy when predicting for X.\n",
    "        '''\n",
    "        # TODO: implement\n",
    "        pass\n",
    "    \n",
    "# TODO: crossvalidation 10 trees, max_depth 10, k = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa9946a",
   "metadata": {},
   "source": [
    "### Task 14\n",
    "\n",
    "Similar to task 7, track the average crossvalidation score over 10 tries for an increasing number of trees in the ensemble.\n",
    "\n",
    "Use $k = 10$ folds and a maximum depth of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976ed3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: increase number of trees, track avg. CV score over 10 tries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
