{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# AGDML-Lab Final\n",
    "Task: Sentiment Analysis of Twitter messages\n",
    "\n",
    "## Preprocessing\n",
    "For preprocessing I used NLTK library. I removed non-alphabetic characters, made words lowercase, removed mentions of other users, removed stopwords and lemmatized each word to its lemma. This should make the data more consistent and easier to work with. My assumption is: most of the spelling mistakes and special characters are unnecessary for sentiment analysis.\n",
    "\n",
    "The regex will most likely match stuff that we do not want removed, but that is a tradeoff we accept."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "N = 1000\n",
    "\n",
    "tqdm.pandas()\n",
    "torch.cuda.get_device_name(0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dataset is evenly balanced. There is no bias towards negative or positive messages."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# preprocessing text messages\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# download stopwords and wordnet\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "nltk.download('sentiwordnet')\n",
    "\n",
    "# create object of WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "# this allows very\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "correct_words = [str.lower(w) for w in words.words()]\n",
    "\n",
    "# BEGIN SOURCE http://norvig.com/spell-correct.html\n",
    "from collections import Counter\n",
    "\n",
    "WORDS = Counter(correct_words)\n",
    "\n",
    "def P(word, N=sum(WORDS.values())):\n",
    "    \"\"\"Probability of `word`.\"\"\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word):\n",
    "    \"\"\"Most probable spelling correction for word.\"\"\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word):\n",
    "    \"\"\"Generate possible spelling corrections for word.\"\"\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words):\n",
    "    \"\"\"The subset of `words` that appear in the dictionary of WORDS.\"\"\"\n",
    "    return set(w for w in words if w in words)\n",
    "\n",
    "def edits1(word):\n",
    "    \"\"\"All edits that are one edit away from `word`.\"\"\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word):\n",
    "    \"\"\"All edits that are two edits away from `word`.\"\"\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "# END SOURCE\n",
    "\n",
    "# function to clean sentences\n",
    "def clean_text(sentence):\n",
    "    # remove mentions of other users\n",
    "    sentence = re.sub('\\B@[._a-zA-Z0-9]{3,24}', '', sentence)\n",
    "    \n",
    "    # rewrite words in all caps to \"very\" followed by word\n",
    "    # text = re.sub('([A-Z]+)', lambda x: 'very ' + x.group(0).lower(), text)\n",
    "    \n",
    "    # make words lowercase, because Go and go will be considered as two words\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    # detect laughing\n",
    "    sentence = re.sub(r'\\b(?:a*(?:ha)+h?|(?:l+o+)+l+)\\b', ' laughing ', sentence)\n",
    "    \n",
    "    # remove multiple dots\n",
    "    sentence = re.sub(r'(\\.)\\1{2,}', '\\1', sentence)\n",
    "    \n",
    "    # remove URLs from text (prefer safely!)\n",
    "    sentence = re.sub('https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,4}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)', ' ', sentence)\n",
    "    \n",
    "    # remove everything but letters\n",
    "    sentence = re.sub('[^a-z]', ' ', sentence)\n",
    "    \n",
    "    # split the sentences into words\n",
    "    _words = sentence.split() \n",
    "    \n",
    "    for i in range(len(_words)):\n",
    "        # remove words with length 1\n",
    "        if len(_words[i]) == 1:\n",
    "            _words[i] = ''\n",
    "    \n",
    "        # remove repetition of letters\n",
    "        _words[i] = re.sub(r'([a-z])\\1{3,}', r'\\1', _words[i])\n",
    "    \n",
    "        # prepend very if there were repeating letters\n",
    "        # if _tmp[1] > 0:\n",
    "        #     words[i] = \"very \" + _tmp[0] // TODO if this is added also add back _tmp variable and rewrite sub to subn\n",
    "    \n",
    "    # remove stopwords like to, and, or etc.\n",
    "    # _words = [word for word in _words if word not in stopwords]\n",
    "    \n",
    "    # spell-check\n",
    "    _words = [correction(word) for word in _words]\n",
    "    \n",
    "    # remove words if they are unknown\n",
    "    # _words = ['' if word not in WORDS else word for word in _words]\n",
    "    \n",
    "    # lemmatize each word\n",
    "    # _words = [wordnet_lemmatizer.lemmatize(word) for word in _words]\n",
    "    \n",
    "    # porter stem word\n",
    "    # _words = [porter_stemmer.stem(word) for word in _words]\n",
    "    \n",
    "    # join words to make sentence\n",
    "    sentence = ' '.join(_words)\n",
    "    # remove multiple spaces\n",
    "    sentence = re.sub('\\s+', ' ', sentence)\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    return sentence"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "to_be_cleaned_examples = [\n",
    "    \"sooooooooooo full .... BBQ was great ..... lovely day ! \",\n",
    "    \"@symphnysldr lets do it\",\n",
    "    \":3 Up and ready for a full day of doing noithing. Apart from finishing new picture, animation, more guitar, tiding my rooms. And homework \",\n",
    "    \"@carswani yeh i need to do another,now that im like u and have sum white face paint..but um..im ok..just tired\",\n",
    "    \"SCOTUS decides that having convicted someone removes their rights to bring evidence that could prove their innocence.  http://tr.im/oXqj\"\n",
    "    \"Finally got my watch fixed... Its only been 7 months.. As me what time it is  http://myloc.me/2Vti\"\n",
    "]\n",
    "cleaned_examples = [clean_text(c) for c in to_be_cleaned_examples]\n",
    "\n",
    "pd.DataFrame({'raw': to_be_cleaned_examples, 'cleaned': cleaned_examples})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# read training data\n",
    "df_clean = pd.read_csv('data.csv')\n",
    "\n",
    "# read validation data\n",
    "df_test_clean = pd.read_csv('data_valid.csv')\n",
    "\n",
    "ct = len(df_clean.loc[df_clean['target'] == 1]) / len(df_clean)\n",
    "\"positive messages\", ct, \"negative messages\", 1-ct "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clean the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# drop rows with missing values\n",
    "df_clean = df_clean.dropna()\n",
    "df_test_clean = df_test_clean.dropna()\n",
    "\n",
    "X_unprocessed = df_clean['text']\n",
    "X_test_unprocessed = df_test_clean['text']\n",
    "\n",
    "# clean text data\n",
    "df_clean['text'] = df_clean['text'].progress_apply(clean_text)\n",
    "df_test_clean['text'] = df_test_clean['text'].progress_apply(clean_text)\n",
    "\n",
    "# drop rows with missing values\n",
    "df_clean = df_clean.dropna()\n",
    "df_test_clean = df_test_clean.dropna()\n",
    "\n",
    "df_clean.to_csv('data_cleaned_no_spell.csv', index=False)    \n",
    "df_test_clean.to_csv('data_valid_cleaned_no_spell.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# remove words which occur less than O times\n",
    "O = 100\n",
    "    \n",
    "training_text = (' '.join(df_clean['text'])).split()\n",
    "test_text = (' '.join(df_test_clean['text'])).split()\n",
    "dataset_counter = Counter(training_text + test_text)\n",
    "\n",
    "dataset_counter.most_common()[-1000:]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def remove_infrequent_words(sentence):\n",
    "    _words = sentence.split()\n",
    "    _words = [word if dataset_counter[word] >= O else ' ' for word in _words]\n",
    "    sentence = ' '.join(_words)\n",
    "    sentence = re.sub('\\s+', ' ', sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence\n",
    "    \n",
    "df_clean['text'] = df_clean['text'].progress_apply(remove_infrequent_words)\n",
    "df_test_clean['text'] = df_test_clean['text'].progress_apply(remove_infrequent_words)\n",
    "\n",
    "df_clean"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "# read cleaned data\n",
    "df = pd.read_csv('data_cleaned.csv')\n",
    "df_test = pd.read_csv('data_valid_cleaned.csv')\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "# read raw data\n",
    "df = pd.read_csv('data.csv')\n",
    "df_test = pd.read_csv('data_valid.csv')\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def drop_not_string(_df, column):\n",
    "#     return _df.drop(_df[_df[column].apply(lambda x: isinstance(x, str)) == False].index)\n",
    "# \n",
    "# drop_not_string(df, 'text')\n",
    "# drop_not_string(df_test, 'text')\n",
    "\n",
    "X_clean = df_clean['text']\n",
    "y = df_clean['target']\n",
    "\n",
    "X_test_clean = df_test_clean['text']\n",
    "\n",
    "# make them all strings\n",
    "X_clean = X_clean.astype(str)\n",
    "X_test_clean = X_test_clean.astype(str)\n",
    "\n",
    "\"sizes:\", X_clean.shape, y.shape, X_test_clean.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## VADER Sentiment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def vader(sentence):\n",
    "    _v = analyzer.polarity_scores(sentence)\n",
    "    return np.array([_v['neg'], _v['neu'], _v['pos'], _v['compound']])\n",
    "\n",
    "_temp = df_clean['text']\n",
    "vader_X = np.array([vader(s) for s in _temp])\n",
    "\n",
    "vader_X_compound = vader_X[:, 3]\n",
    "vader_X = vader_X[:, [0, 1, 2]]\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "vader_X_poly = PolynomialFeatures().fit_transform(vader_X, y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "_, _X_test, _, _y_test = train_test_split(X_unprocessed, y, test_size=0.33)\n",
    "_y_hat_test = np.array([vader(x)[3] >= 0 for x in _X_test])\n",
    "\"accuracy vader on unprocessed\", len(np.where(_y_hat_test == _y_test)[0])/len(_y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_, _X_test, _, _y_test = train_test_split(X_clean, y, test_size=0.33)\n",
    "_y_hat_test = np.array([vader(x)[3] >= 0 for x in _X_test])\n",
    "\"accuracy vader on unprocessed\", len(np.where(_y_hat_test == _y_test)[0])/len(_y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vader(\"going to save up for new camera wish me luck\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### remove neutral words\n",
    "We remove neutral words (below a certain threshold) to reduce complexity. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "analyzer.polarity_scores(\"bias\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def remove_neutral(sentence):\n",
    "    _words = sentence.split()\n",
    "    _words = [w if vader(w)[1] <= 0.1 else ' ' for w in _words]\n",
    "    sentence = ' '.join(_words)\n",
    "    # remove multiple spaces\n",
    "    sentence = re.sub('\\s+', ' ', sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence\n",
    "\n",
    "df_clean['text'] = df_clean['text'].progress_apply(remove_neutral)\n",
    "df_test_clean['text'] = df_test_clean['text'].progress_apply(remove_neutral)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word2Vec\n",
    "Using the Gensim implementation of Google's Word2Vec.\n",
    "Firstly we train it on cleaned data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "def w2v_map(_X, _X_test):\n",
    "    sentences = [sentence.split() for sentence in _X]\n",
    "    w2v_model = Word2Vec(sentences, window=5, min_count=5, workers=4, hs=1 , negative=0)\n",
    "    \n",
    "    def vectorize(sentence):\n",
    "        words = sentence.split()\n",
    "        words_vecs = [w2v_model.wv[word] for word in words if word in w2v_model.wv]\n",
    "        if len(words_vecs) == 0:\n",
    "            return np.zeros(100)\n",
    "        words_vecs = np.array(words_vecs)\n",
    "        return words_vecs.mean(axis=0)\n",
    "    \n",
    "    X_clean_w2v = np.array([vectorize(sentence) for sentence in tqdm(_X)])\n",
    "    X_clean_test_w2v = np.array([vectorize(sentence) for sentence in tqdm(_X)])\n",
    "    \n",
    "    return X_clean_w2v, X_clean_test_w2v\n",
    "\n",
    "X_clean_w2v, X_clean_test_w2v = w2v_map(X_clean, X_test_clean)\n",
    "X_clean_w2v"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then also uncleaned data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_unprocessed_w2v, X_test_unprocessed_w2v = w2v_map(X_unprocessed, X_test_unprocessed)\n",
    "X_unprocessed_w2v"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## combine w2v and VADER"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_w2v_vader = np.copy(X_clean_w2v)\n",
    "aggression_factor = 100\n",
    "\n",
    "for i in range(X_clean_w2v.shape[0]):\n",
    "    X_w2v_vader[i] = X_clean_w2v[i] + (aggression_factor * vader_X_compound[i]) \n",
    "X_w2v_vader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Mapping Visualisation\n",
    "The 100-dimensional Word2Vec mapping of our features is reduced to a 2-dimensional space. The colouring is according to the label."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import umap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "reducer = umap.UMAP()\n",
    "\n",
    "scaled_we = StandardScaler().fit(X_unprocessed_w2v)\n",
    "\n",
    "tf_X_w2v = scaled_we.transform(X_clean_w2v)[:N]\n",
    "tf_X_w2v_vader = scaled_we.transform(X_w2v_vader)[:N]\n",
    "tf_X_unprocessed_w2v = scaled_we.transform(X_unprocessed_w2v)[:N]\n",
    "\n",
    "reducer.fit(tf_X_w2v)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_embedding1 = reducer.transform(tf_X_w2v)\n",
    "_embedding2 = reducer.transform(tf_X_w2v_vader)\n",
    "_embedding3 = reducer.transform(tf_X_unprocessed_w2v)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### unprocessed w2v"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(\n",
    "    _embedding3[:, 0],\n",
    "    _embedding3[:, 1],\n",
    "    color=['r' if _y == 0 else 'b' for _y in y[:N]],\n",
    "    alpha=0.2\n",
    ")\n",
    "plt.savefig('unprocessed_w2v.png')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### clean w2v"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(\n",
    "    _embedding1[:, 0],\n",
    "    _embedding1[:, 1],\n",
    "    color=['r' if _y == 0 else 'b' for _y in y[:N]],\n",
    "    alpha=0.2\n",
    ")\n",
    "plt.ylim(40, 50)\n",
    "plt.xlim(-24, -13)\n",
    "plt.savefig('clean_w2v.png')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### w2v with VADER"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.xlim(-23, -14)\n",
    "plt.ylim(39, 50)\n",
    "plt.scatter(\n",
    "    _embedding2[:, 0],\n",
    "    _embedding2[:, 1],\n",
    "    color=['g' if _y == 0 else 'b' for _y in y[:N]],\n",
    "    alpha=0.2\n",
    ")\n",
    "plt.savefig('w2v_x_vader.png')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classifiers\n",
    "This compares different classifiers. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, LinearRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "\n",
    "\n",
    "embeddings = [\n",
    "    {\"name\": \"w2v clean\", \"data\": X_clean_w2v},\n",
    "    # {\"name\": \"w2v unprocessed\", \"data\": X_unprocessed_w2v},\n",
    "    # {\"name\": \"vader\", \"data\": vader_X},\n",
    "    # {\"name\": \"vader x w2v\", \"data\": X_w2v_vader},\n",
    "    # {\"name\": \"vader poly\", \"data\": vader_X_poly}\n",
    "]\n",
    "classifiers = [\n",
    "    {\"name\": \"LogReg\", \"cf\": LogisticRegression(max_iter=10000)},\n",
    "    # {\"name\": \"Ridge\", \"cf\": RidgeClassifier(max_iter=10000)},\n",
    "    # {\"name: \"Gauss\", \"cf\": GaussianNB()},\n",
    "    # {\"name\": \"DecisionTree depth=10\", \"cf\": DecisionTreeClassifier(max_depth=10), # train=0.71, test=0.68 on w2v clean\n",
    "    {\"name\": \"RandomForest estimators=10\", \"cf\": RandomForestClassifier(n_estimators=10, n_jobs=-1)},\n",
    "    # {\"name\": \"RandomForest estimators=100\", \"cf\": RandomForestClassifier(n_estimators=100, n_jobs=-1)},\n",
    "    # {\"name\": \"RandomForest estimators=500\", \"cf\": RandomForestClassifier(n_estimators=500, n_jobs=-1)},\n",
    "    {\"name\": \"Bagging max_samples=0.5, estimators=10\", \"cf\": BaggingClassifier(DecisionTreeClassifier(), max_samples=0.5, max_features=0.5, n_jobs=-1)},\n",
    "    # {\"name\": \"AdaBoostClassifier(n_estimators=10)\", \"cf\": AdaBoostClassifier(n_estimators=10)},\n",
    "    # {\"name\": \"AdaBoostClassifier(n_estimators=50)\", \"cf\": AdaBoostClassifier(n_estimators=50)},\n",
    "    # {\"name\": \"GradientBoosting estimators=100\", \"cf\": GradientBoostingClassifier(n_estimators=100)},\n",
    "    # {\"name\": \"GradientBoosting estimators=10\", \"cf\": GradientBoostingClassifier(n_estimators=10)},\n",
    "    # {\"name\": \"ExtraTrees estimators=10\", \"cf\": ExtraTreesClassifier(n_estimators=10, n_jobs=-1)},\n",
    "    {\"name\": \"ExtraTrees esimators=500\", \"cf\": ExtraTreesClassifier(n_estimators=500, n_jobs=-1)},\n",
    "    # {\"name\": \"ExtraTrees estimators=1000\", \"cf\": ExtraTreesClassifier(n_estimators=1000, n_jobs=-1)},\n",
    "]\n",
    "\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "e_o = []\n",
    "c_o = []\n",
    "    \n",
    "for e in tqdm(embeddings, desc=\"embedding\", position=0, leave=False):\n",
    "    _X_train, _X_test, _y_train, _y_test = train_test_split(e[\"data\"], y, test_size=0.33)\n",
    "    \n",
    "    for c in tqdm(classifiers, desc=\"classifier\", position=1, leave=False):\n",
    "        c[\"cf\"].fit(_X_train, _y_train)\n",
    "        _y_hat_train = c[\"cf\"].predict(_X_train)\n",
    "        _y_hat_test = c[\"cf\"].predict(_X_test)\n",
    "        \n",
    "        e_o.append(e[\"name\"])\n",
    "        c_o.append(c[\"name\"])\n",
    "        train_acc.append(accuracy_score(_y_hat_train, _y_train))\n",
    "        test_acc.append(accuracy_score(_y_hat_test, _y_test))\n",
    "       \n",
    "pd.DataFrame({'embedding': e_o, 'classifier': c_o, 'train accuracy': train_acc, 'test accuracy': test_acc})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.DataFrame({'embedding': e_o, 'classifier': c_o, 'train accuracy': train_acc, 'test accuracy': test_acc})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save prediction with Logistic Regression to file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def save_pred_to_file(cf, _X_train, _y_train, _X_validation):\n",
    "    cf.fit(_X_train, _y_train)\n",
    "    y_pred = cf.predict(_X_train)\n",
    "    print(accuracy_score(y, y_pred))\n",
    "    \n",
    "    df_clean.filter(items=np.where(y_pred != _y_train)[0], axis=0).to_csv(\"mismatched.csv\")\n",
    "    \n",
    "    y_pred_test = cf.predict(_X_validation)\n",
    "    np.save('y_pred.npy', y_pred_test)\n",
    "\n",
    "cf = LogisticRegression()\n",
    "save_pred_to_file(cf, X_clean_w2v, y, X_clean_test_w2v)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.load(\"y_pred.npy\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transformer\n",
    "Using a pre-trained transformer for sentiment analysis from Huggingface.\n",
    "Score on the validation set _unprocessed_: 0.71\n",
    "cleaned: 0.66\n",
    "\n",
    "This method is therefore worse on unprocessed data than Logistic Regression with w2v."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "from transformers import pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "y_pred_transformer = sentiment_pipeline(df_test_clean['text'].tolist())\n",
    "\n",
    "y_pred_transformer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "y_pred_transformer = np.array([1 if x['label'] == 'POSITIVE' else 0 for x in y_pred_transformer])\n",
    "np.save('y_pred_transformer.npy', y_pred_transformer)\n",
    "np.array(y_pred_transformer).shape[0] == len(df_test_clean)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
