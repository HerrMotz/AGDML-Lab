{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# AGDML-Lab Final\n",
    "Task: Sentiment Analysis of Twitter messages\n",
    "\n",
    "## Preprocessing\n",
    "For preprocessing I used NLTK library. I removed non-alphabetic characters, made words lowercase, removed mentions of other users, removed stopwords and lemmatized each word to its lemma. This should make the data more consistent and easier to work with. My assumption is: most of the spelling mistakes and special characters are unnecessary for sentiment analysis.\n",
    "\n",
    "The regex will most likely match stuff that we do not want removed, but that is a tradeoff we accept."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "'NVIDIA GeForce GTX 1060 6GB'"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "N = 1000\n",
    "\n",
    "tqdm.pandas()\n",
    "torch.cuda.get_device_name(0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-28T19:14:43.975734100Z",
     "start_time": "2023-07-28T19:14:43.970742300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dataset is evenly balanced. There is no bias towards negative or positive messages."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ours', 'you', 'this', 'both', \"wasn't\", \"haven't\", 'and', 'just', 'y', 'against', 'some', 'wasn', 'below', 'll', 'when', 'its', 'too', 'so', 'myself', 'or', 'now', \"you'll\", \"hadn't\", 'o', 'once', 'we', 'other', 'should', 'most', 'before', 'won', 'herself', 'why', \"mightn't\", 'aren', 'm', 'isn', 'that', 'while', 'during', 'from', 'through', 'about', \"weren't\", 'above', \"aren't\", 'but', 'd', 're', 'all', \"you've\", 't', 'hadn', 'them', \"needn't\", 'those', 'by', 'what', 'weren', 'each', 'doesn', 'it', 'here', \"you'd\", 'only', 'up', 'not', 'own', 'having', 's', 'wouldn', 'does', 'again', 'hasn', 'after', 'theirs', 'no', 'ma', 'where', 'ourselves', 'down', 'can', 'few', \"should've\", 'who', 'ain', 'are', 'their', 'same', 'themselves', 'the', 'until', 'how', 'under', 'as', 'did', 've', 'my', 'had', 'yours', 'in', \"couldn't\", 'was', 'been', 'shan', \"that'll\", \"didn't\", \"wouldn't\", \"she's\", \"it's\", 'were', 'because', \"won't\", 'he', 'off', 'she', 'i', 'shouldn', 'for', 'on', 'with', 'an', 'if', 'there', \"hasn't\", 'mightn', 'haven', 'me', 'hers', 'than', \"isn't\", 'out', 'which', \"doesn't\", 'do', 'am', 'don', \"mustn't\", 'doing', \"shouldn't\", 'your', 'whom', 'any', \"shan't\", 'these', 'didn', 'further', 'be', 'then', 'between', 'couldn', 'him', 'her', 'has', 'being', 'into', 'a', 'of', 'over', 'mustn', 'they', \"don't\", 'will', 'have', 'nor', 'his', 'yourself', 'itself', 'needn', 'is', 'himself', 'to', 'such', 'our', 'at', \"you're\", 'yourselves', 'more'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\frand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\frand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\frand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# preprocessing text messages\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# download stopwords and wordnet\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "\n",
    "# create object of WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# this allows very\n",
    "stopwords = set(stopwords.words('english'))\n",
    "stopwords.remove('very')\n",
    "print(stopwords)\n",
    "\n",
    "correct_words = [str.lower(w) for w in words.words()]\n",
    "\n",
    "# BEGIN SOURCE http://norvig.com/spell-correct.html\n",
    "from collections import Counter\n",
    "\n",
    "WORDS = Counter(correct_words)\n",
    "\n",
    "def P(word, N=sum(WORDS.values())):\n",
    "    \"\"\"Probability of `word`.\"\"\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word):\n",
    "    \"\"\"Most probable spelling correction for word.\"\"\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word):\n",
    "    \"\"\"Generate possible spelling corrections for word.\"\"\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words):\n",
    "    \"\"\"The subset of `words` that appear in the dictionary of WORDS.\"\"\"\n",
    "    return set(w for w in words if w in words)\n",
    "\n",
    "def edits1(word):\n",
    "    \"\"\"All edits that are one edit away from `word`.\"\"\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word):\n",
    "    \"\"\"All edits that are two edits away from `word`.\"\"\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "# END SOURCE\n",
    "\n",
    "# function to clean sentences\n",
    "def clean_text(sentence):\n",
    "    # remove mentions of other users\n",
    "    sentence = re.sub('\\B@[._a-zA-Z0-9]{3,24}', '', sentence)\n",
    "    \n",
    "    # rewrite words in all caps to \"very\" followed by word\n",
    "    # text = re.sub('([A-Z]+)', lambda x: 'very ' + x.group(0).lower(), text)\n",
    "    \n",
    "    # make words lowercase, because Go and go will be considered as two words\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    # detect laughing\n",
    "    sentence = re.sub(r'\\b(?:a*(?:ha)+h?|(?:l+o+)+l+)\\b', ' laughing ', sentence)\n",
    "    \n",
    "    # remove multiple dots\n",
    "    sentence = re.sub(r'(\\.)\\1{2,}', '\\1', sentence)\n",
    "    \n",
    "    # remove URLs from text (prefer safely!)\n",
    "    sentence = re.sub('https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,4}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)', ' ', sentence)\n",
    "    \n",
    "    # remove everything but letters\n",
    "    sentence = re.sub('[^a-z]', ' ', sentence)\n",
    "    \n",
    "    # split the sentences into words\n",
    "    _words = sentence.split() \n",
    "    \n",
    "    for i in range(len(_words)):\n",
    "        # remove words with length 1\n",
    "        if len(_words[i]) == 1:\n",
    "            _words[i] = ''\n",
    "    \n",
    "        # remove repetition of letters\n",
    "        _words[i] = re.sub(r'([a-z])\\1{3,}', r'\\1', _words[i])\n",
    "    \n",
    "        # prepend very if there were repeating letters\n",
    "        # if _tmp[1] > 0:\n",
    "        #     words[i] = \"very \" + _tmp[0] // TODO if this is added also add back _tmp variable and rewrite sub to subn\n",
    "    \n",
    "    # remove stopwords like to, and, or etc.\n",
    "    # _words = [word for word in _words if word not in stopwords]\n",
    "    \n",
    "    # spell-check\n",
    "    _words = [correction(word) for word in _words]\n",
    "    \n",
    "    # remove words if they are unknown\n",
    "    # _words = ['' if word not in WORDS else word for word in _words]\n",
    "    \n",
    "    # lemmatize each word\n",
    "    # _words = [wordnet_lemmatizer.lemmatize(word) for word in _words]\n",
    "    \n",
    "    # join words to make sentence\n",
    "    sentence = ' '.join(_words)\n",
    "    # remove multiple spaces\n",
    "    sentence = re.sub('\\s+', ' ', sentence)\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    return sentence"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-28T19:10:23.691692900Z",
     "start_time": "2023-07-28T19:10:23.490076200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                 raw   \n0  sooooooooooo full .... BBQ was great ..... lov...  \\\n1                            @symphnysldr lets do it   \n2  :3 Up and ready for a full day of doing noithi...   \n3  @carswani yeh i need to do another,now that im...   \n4  SCOTUS decides that having convicted someone r...   \n\n                                             cleaned  \n0                   so full bbq was great lovely day  \n1                                         lets do it  \n2  up and ready for full day of doing noithing ap...  \n3  yeh need to do another now that im like and ha...  \n4  scotus decides that having convicted someone r...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>raw</th>\n      <th>cleaned</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>sooooooooooo full .... BBQ was great ..... lov...</td>\n      <td>so full bbq was great lovely day</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>@symphnysldr lets do it</td>\n      <td>lets do it</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>:3 Up and ready for a full day of doing noithi...</td>\n      <td>up and ready for full day of doing noithing ap...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>@carswani yeh i need to do another,now that im...</td>\n      <td>yeh need to do another now that im like and ha...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>SCOTUS decides that having convicted someone r...</td>\n      <td>scotus decides that having convicted someone r...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_be_cleaned_examples = [\n",
    "    \"sooooooooooo full .... BBQ was great ..... lovely day ! \",\n",
    "    \"@symphnysldr lets do it\",\n",
    "    \":3 Up and ready for a full day of doing noithing. Apart from finishing new picture, animation, more guitar, tiding my rooms. And homework \",\n",
    "    \"@carswani yeh i need to do another,now that im like u and have sum white face paint..but um..im ok..just tired\",\n",
    "    \"SCOTUS decides that having convicted someone removes their rights to bring evidence that could prove their innocence.  http://tr.im/oXqj\"\n",
    "]\n",
    "cleaned_examples = [clean_text(c) for c in to_be_cleaned_examples]\n",
    "\n",
    "pd.DataFrame({'raw': to_be_cleaned_examples, 'cleaned': cleaned_examples})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-28T19:10:56.066179700Z",
     "start_time": "2023-07-28T19:10:56.055949200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/500000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ba2da180fcd9400bab06066a1f1a5a01"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/100000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6966c8a9cde14ca39d5fb85f7a6911b1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# read training data\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# read validation data\n",
    "df_test = pd.read_csv('data_valid.csv')\n",
    "\n",
    "ct = len(df.loc[df['target'] == 1]) / len(df)\n",
    "\"positive messages\", ct, \"negative messages\", 1-ct \n",
    "\n",
    "# drop rows with missing values\n",
    "df = df.dropna()\n",
    "df_test = df_test.dropna()\n",
    "\n",
    "# clean text data\n",
    "df['text'] = df['text'].progress_apply(clean_text)\n",
    "df_test['text'] = df_test['text'].progress_apply(clean_text)\n",
    "\n",
    "# drop rows with missing values\n",
    "df = df.dropna()\n",
    "df_test = df_test.dropna()\n",
    "\n",
    "# df.to_csv('data_cleaned.csv', index=False)    \n",
    "# df_test.to_csv('data_valid_cleaned.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-28T19:34:59.840803300Z",
     "start_time": "2023-07-28T19:34:28.401500400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "[('to', 212937),\n ('the', 196519),\n ('my', 118867),\n ('it', 114797),\n ('and', 113775),\n ('you', 113406),\n ('is', 89728),\n ('in', 81290),\n ('for', 81198),\n ('of', 68823),\n ('on', 63718),\n ('me', 62513),\n ('that', 61919),\n ('so', 60416),\n ('have', 54675),\n ('but', 50036),\n ('just', 47506),\n ('with', 43076),\n ('laughing', 42948),\n ('be', 42390),\n ('at', 42092),\n ('not', 40608),\n ('was', 39269),\n ('can', 35401),\n ('this', 35337),\n ('now', 34985),\n ('up', 34192),\n ('good', 34120),\n ('day', 33656),\n ('all', 31339),\n ('get', 30875),\n ('out', 30779),\n ('no', 29623),\n ('like', 29427),\n ('are', 29129),\n ('go', 27819),\n ('quot', 26986),\n ('http', 26572),\n ('today', 25657),\n ('do', 25268),\n ('too', 24790),\n ('love', 24675),\n ('work', 24466),\n ('going', 24464),\n ('your', 24389),\n ('we', 24024),\n ('got', 23116),\n ('what', 22275),\n ('am', 21844),\n ('time', 21702),\n ('back', 21455),\n ('from', 21042),\n ('one', 20438),\n ('com', 19742),\n ('will', 19708),\n ('know', 19696),\n ('im', 19629),\n ('about', 18884),\n ('really', 18859),\n ('don', 18813),\n ('amp', 18224),\n ('had', 17893),\n ('see', 17460),\n ('some', 17210),\n ('its', 16846),\n ('there', 16818),\n ('night', 16618),\n ('how', 16534),\n ('if', 16409),\n ('still', 16331),\n ('they', 16057),\n ('well', 16049),\n ('new', 15873),\n ('want', 15850),\n ('think', 15514),\n ('oh', 15443),\n ('ll', 15194),\n ('thanks', 15174),\n ('home', 15081),\n ('when', 14922),\n ('as', 14654),\n ('he', 14628),\n ('more', 14348),\n ('here', 14284),\n ('off', 13908),\n ('much', 13795),\n ('miss', 13582),\n ('last', 13445),\n ('need', 13426),\n ('re', 13267),\n ('an', 13266),\n ('then', 13038),\n ('morning', 12911),\n ('tomorrow', 12816),\n ('great', 12746),\n ('hope', 12647),\n ('twitter', 12506),\n ('been', 12347),\n ('has', 12265),\n ('or', 11956),\n ('again', 11703),\n ('her', 11622),\n ('feel', 11155),\n ('sad', 11056),\n ('she', 11011),\n ('fun', 10831),\n ('why', 10803),\n ('sleep', 10721),\n ('wish', 10562),\n ('only', 10557),\n ('right', 10430),\n ('bad', 10275),\n ('very', 10262),\n ('would', 10215),\n ('happy', 10143),\n ('tonight', 9876),\n ('sorry', 9859),\n ('ve', 9850),\n ('did', 9671),\n ('by', 9469),\n ('come', 9372),\n ('make', 9349),\n ('them', 9282),\n ('way', 9272),\n ('getting', 9206),\n ('bit', 9036),\n ('over', 8974),\n ('gonna', 8960),\n ('though', 8935),\n ('better', 8835),\n ('nice', 8767),\n ('watching', 8607),\n ('yeah', 8496),\n ('bed', 8443),\n ('should', 8403),\n ('wait', 8368),\n ('could', 8346),\n ('week', 8142),\n ('school', 7855),\n ('twitpic', 7748),\n ('didn', 7702),\n ('people', 7701),\n ('hate', 7476),\n ('him', 7439),\n ('days', 7352),\n ('hey', 7331),\n ('even', 7259),\n ('yes', 7186),\n ('who', 7104),\n ('after', 7048),\n ('lt', 7030),\n ('next', 6966),\n ('down', 6947),\n ('awesome', 6888),\n ('dont', 6869),\n ('weekend', 6816),\n ('thank', 6813),\n ('never', 6784),\n ('cant', 6709),\n ('were', 6638),\n ('soon', 6623),\n ('long', 6518),\n ('little', 6381),\n ('take', 6340),\n ('working', 6340),\n ('please', 6330),\n ('show', 6297),\n ('wanna', 6293),\n ('tired', 6286),\n ('first', 6274),\n ('best', 6270),\n ('say', 6182),\n ('ok', 6124),\n ('everyone', 6112),\n ('sick', 6103),\n ('doing', 6075),\n ('life', 6012),\n ('having', 5998),\n ('being', 5995),\n ('watch', 5962),\n ('his', 5891),\n ('done', 5805),\n ('any', 5802),\n ('our', 5771),\n ('feeling', 5712),\n ('sure', 5629),\n ('us', 5608),\n ('let', 5596),\n ('where', 5590),\n ('already', 5588),\n ('thing', 5575),\n ('always', 5551),\n ('friends', 5533),\n ('cool', 5520),\n ('yay', 5509),\n ('find', 5481),\n ('another', 5481),\n ('something', 5468),\n ('than', 5432),\n ('won', 5418),\n ('man', 5406),\n ('guys', 5326),\n ('ready', 5290),\n ('made', 5229),\n ('ly', 5204),\n ('looking', 5130),\n ('phone', 5128),\n ('because', 5117),\n ('look', 5084),\n ('hours', 5039),\n ('yet', 5004),\n ('ur', 4918),\n ('went', 4908),\n ('before', 4905),\n ('house', 4886),\n ('ever', 4876),\n ('pretty', 4765),\n ('trying', 4751),\n ('tweet', 4750),\n ('movie', 4749),\n ('maybe', 4698),\n ('away', 4649),\n ('summer', 4563),\n ('damn', 4546),\n ('finally', 4516),\n ('omg', 4511),\n ('help', 4456),\n ('amazing', 4448),\n ('early', 4431),\n ('follow', 4405),\n ('old', 4348),\n ('lost', 4345),\n ('things', 4332),\n ('guess', 4326),\n ('into', 4307),\n ('friend', 4283),\n ('left', 4269),\n ('someone', 4246),\n ('keep', 4245),\n ('wow', 4227),\n ('rain', 4214),\n ('same', 4204),\n ('big', 4192),\n ('thought', 4191),\n ('year', 4173),\n ('ya', 4170),\n ('missed', 4170),\n ('ugh', 4135),\n ('nothing', 4134),\n ('sucks', 4116),\n ('hot', 4088),\n ('while', 4046),\n ('bored', 4016),\n ('other', 3996),\n ('god', 3979),\n ('girl', 3974),\n ('live', 3964),\n ('start', 3958),\n ('baby', 3957),\n ('try', 3952),\n ('birthday', 3931),\n ('also', 3930),\n ('doesn', 3929),\n ('glad', 3913),\n ('two', 3910),\n ('does', 3886),\n ('looks', 3879),\n ('coming', 3872),\n ('tell', 3864),\n ('later', 3863),\n ('sun', 3859),\n ('actually', 3820),\n ('weather', 3820),\n ('party', 3803),\n ('song', 3793),\n ('excited', 3776),\n ('hear', 3773),\n ('saw', 3769),\n ('those', 3755),\n ('stuff', 3746),\n ('th', 3740),\n ('said', 3705),\n ('hard', 3697),\n ('play', 3687),\n ('makes', 3643),\n ('waiting', 3638),\n ('might', 3634),\n ('hi', 3583),\n ('until', 3581),\n ('world', 3556),\n ('since', 3547),\n ('yesterday', 3541),\n ('game', 3539),\n ('thats', 3515),\n ('www', 3483),\n ('late', 3483),\n ('few', 3482),\n ('gotta', 3477),\n ('lot', 3475),\n ('myself', 3438),\n ('around', 3404),\n ('such', 3400),\n ('music', 3367),\n ('friday', 3339),\n ('many', 3333),\n ('mom', 3323),\n ('car', 3321),\n ('must', 3302),\n ('sounds', 3287),\n ('call', 3281),\n ('read', 3269),\n ('job', 3269),\n ('luck', 3263),\n ('check', 3252),\n ('found', 3234),\n ('head', 3231),\n ('sunday', 3212),\n ('their', 3212),\n ('give', 3190),\n ('haven', 3184),\n ('beautiful', 3184),\n ('monday', 3177),\n ('missing', 3160),\n ('making', 3145),\n ('gone', 3143),\n ('cold', 3134),\n ('aww', 3125),\n ('may', 3123),\n ('anything', 3108),\n ('put', 3093),\n ('poor', 3059),\n ('okay', 3056),\n ('stop', 3037),\n ('leave', 3031),\n ('talk', 3029),\n ('ah', 3014),\n ('woke', 3001),\n ('almost', 3000),\n ('iphone', 2992),\n ('most', 2982),\n ('least', 2965),\n ('use', 2960),\n ('hair', 2959),\n ('hour', 2951),\n ('tho', 2925),\n ('free', 2919),\n ('till', 2916),\n ('hurts', 2905),\n ('family', 2901),\n ('isn', 2901),\n ('far', 2901),\n ('lunch', 2889),\n ('wanted', 2887),\n ('times', 2884),\n ('everything', 2865),\n ('cute', 2864),\n ('listening', 2864),\n ('dinner', 2843),\n ('end', 2838),\n ('eat', 2837),\n ('finished', 2836),\n ('anyone', 2827),\n ('mean', 2824),\n ('welcome', 2818),\n ('food', 2810),\n ('thinking', 2797),\n ('funny', 2783),\n ('enjoy', 2778),\n ('sweet', 2773),\n ('followers', 2767),\n ('playing', 2764),\n ('forward', 2760),\n ('gt', 2755),\n ('without', 2752),\n ('believe', 2740),\n ('shit', 2736),\n ('which', 2699),\n ('video', 2669),\n ('cause', 2667),\n ('every', 2648),\n ('aw', 2642),\n ('these', 2605),\n ('real', 2593),\n ('tinyurl', 2570),\n ('totally', 2568),\n ('stupid', 2566),\n ('mine', 2566),\n ('outside', 2555),\n ('enough', 2548),\n ('didnt', 2547),\n ('wrong', 2532),\n ('through', 2528),\n ('buy', 2522),\n ('ill', 2506),\n ('probably', 2501),\n ('tv', 2499),\n ('coffee', 2497),\n ('place', 2490),\n ('weeks', 2488),\n ('room', 2485),\n ('anymore', 2477),\n ('once', 2447),\n ('stay', 2423),\n ('win', 2402),\n ('dad', 2398),\n ('money', 2392),\n ('eating', 2370),\n ('tweets', 2358),\n ('following', 2355),\n ('plurk', 2345),\n ('busy', 2332),\n ('wants', 2313),\n ('lovely', 2311),\n ('pic', 2297),\n ('sooo', 2280),\n ('saturday', 2273),\n ('seen', 2262),\n ('kinda', 2252),\n ('taking', 2251),\n ('xx', 2251),\n ('kids', 2245),\n ('came', 2243),\n ('class', 2241),\n ('says', 2232),\n ('crazy', 2231),\n ('whole', 2226),\n ('super', 2214),\n ('hopefully', 2197),\n ('both', 2192),\n ('news', 2178),\n ('beach', 2174),\n ('headache', 2169),\n ('book', 2149),\n ('hello', 2143),\n ('post', 2143),\n ('half', 2140),\n ('exam', 2137),\n ('name', 2129),\n ('guy', 2123),\n ('years', 2114),\n ('took', 2113),\n ('idea', 2108),\n ('awww', 2106),\n ('meet', 2103),\n ('full', 2101),\n ('hell', 2085),\n ('face', 2068),\n ('pm', 2059),\n ('shopping', 2053),\n ('goodnight', 2052),\n ('leaving', 2038),\n ('able', 2036),\n ('forgot', 2034),\n ('sitting', 2029),\n ('wasn', 2027),\n ('either', 2026),\n ('hurt', 2024),\n ('computer', 2022),\n ('true', 2019),\n ('reading', 2018),\n ('run', 2005),\n ('rest', 1998),\n ('send', 1996),\n ('girls', 1994),\n ('soo', 1981),\n ('else', 1980),\n ('couldn', 1975),\n ('ago', 1974),\n ('boo', 1964),\n ('cuz', 1961),\n ('lots', 1953),\n ('raining', 1946),\n ('blog', 1916),\n ('fuck', 1914),\n ('feels', 1909),\n ('used', 1906),\n ('office', 1903),\n ('heart', 1902),\n ('btw', 1896),\n ('tried', 1890),\n ('heard', 1887),\n ('remember', 1886),\n ('mind', 1885),\n ('own', 1880),\n ('alone', 1873),\n ('watched', 1871),\n ('stuck', 1867),\n ('talking', 1863),\n ('dog', 1859),\n ('hit', 1853),\n ('seems', 1850),\n ('internet', 1845),\n ('needs', 1836),\n ('seeing', 1833),\n ('hehe', 1827),\n ('boy', 1825),\n ('st', 1820),\n ('trip', 1818),\n ('course', 1811),\n ('quite', 1808),\n ('facebook', 1798),\n ('part', 1792),\n ('started', 1780),\n ('picture', 1777),\n ('using', 1769),\n ('awake', 1767),\n ('fine', 1760),\n ('add', 1755),\n ('online', 1751),\n ('boring', 1750),\n ('pain', 1748),\n ('kind', 1746),\n ('break', 1736),\n ('cry', 1730),\n ('nite', 1715),\n ('goes', 1710),\n ('loved', 1698),\n ('pics', 1696),\n ('breakfast', 1696),\n ('told', 1683),\n ('wont', 1677),\n ('til', 1674),\n ('asleep', 1673),\n ('bought', 1671),\n ('crap', 1658),\n ('dude', 1658),\n ('sunny', 1655),\n ('broke', 1646),\n ('update', 1641),\n ('change', 1641),\n ('minutes', 1638),\n ('called', 1633),\n ('link', 1632),\n ('june', 1628),\n ('lmao', 1625),\n ('hungry', 1625),\n ('lucky', 1624),\n ('starting', 1623),\n ('ass', 1623),\n ('seriously', 1617),\n ('care', 1617),\n ('la', 1617),\n ('concert', 1616),\n ('season', 1613),\n ('open', 1609),\n ('month', 1603),\n ('sister', 1601),\n ('person', 1597),\n ('anyway', 1592),\n ('gets', 1586),\n ('pay', 1585),\n ('wake', 1584),\n ('bye', 1577),\n ('bring', 1573),\n ('train', 1565),\n ('site', 1556),\n ('heading', 1553),\n ('drive', 1546),\n ('sleeping', 1541),\n ('jealous', 1541),\n ('text', 1539),\n ('yea', 1535),\n ('fan', 1533),\n ('red', 1530),\n ('instead', 1530),\n ('favorite', 1525),\n ('afternoon', 1520),\n ('shower', 1519),\n ('xd', 1514),\n ('study', 1505),\n ('reply', 1501),\n ('walk', 1501),\n ('rock', 1500),\n ('youtube', 1490),\n ('brother', 1490),\n ('ice', 1486),\n ('exams', 1485),\n ('hoping', 1481),\n ('mother', 1474),\n ('enjoying', 1468),\n ('bout', 1465),\n ('together', 1458),\n ('mad', 1456),\n ('high', 1455),\n ('running', 1455),\n ('wonderful', 1451),\n ('congrats', 1440),\n ('fail', 1431),\n ('sometimes', 1429),\n ('died', 1427),\n ('finish', 1426),\n ('dead', 1425),\n ('email', 1418),\n ('sigh', 1414),\n ('move', 1413),\n ('laptop', 1411),\n ('town', 1407),\n ('definitely', 1403),\n ('means', 1401),\n ('city', 1399),\n ('goin', 1397),\n ('sore', 1396),\n ('happened', 1396),\n ('problem', 1392),\n ('fm', 1391),\n ('homework', 1383),\n ('boys', 1382),\n ('dear', 1381),\n ('album', 1379),\n ('top', 1376),\n ('couple', 1373),\n ('movies', 1372),\n ('ask', 1370),\n ('fucking', 1362),\n ('suck', 1356),\n ('comes', 1352),\n ('months', 1351),\n ('church', 1349),\n ('works', 1349),\n ('dream', 1346),\n ('less', 1342),\n ('studying', 1342),\n ('eyes', 1341),\n ('drink', 1339),\n ('set', 1336),\n ('ipod', 1334),\n ('meeting', 1333),\n ('tea', 1325),\n ('happen', 1320),\n ('ppl', 1318),\n ('weird', 1317),\n ('reason', 1314),\n ('write', 1314),\n ('tour', 1311),\n ('listen', 1309),\n ('ahh', 1307),\n ('water', 1307),\n ('perfect', 1304),\n ('loves', 1304),\n ('ive', 1303),\n ('evening', 1301),\n ('songs', 1300),\n ('cut', 1299),\n ('close', 1299),\n ('fall', 1298),\n ('cat', 1294),\n ('lil', 1288),\n ('loving', 1272),\n ('seem', 1271),\n ('cream', 1270),\n ('ones', 1268),\n ('nd', 1264),\n ('fb', 1263),\n ('store', 1262),\n ('final', 1255),\n ('interesting', 1255),\n ('visit', 1253),\n ('dance', 1253),\n ('gym', 1252),\n ('side', 1251),\n ('catch', 1251),\n ('hang', 1244),\n ('nap', 1242),\n ('test', 1240),\n ('sound', 1240),\n ('second', 1239),\n ('wishing', 1239),\n ('clean', 1238),\n ('ate', 1236),\n ('page', 1231),\n ('english', 1224),\n ('knew', 1222),\n ('agree', 1221),\n ('moment', 1219),\n ('story', 1217),\n ('pool', 1217),\n ('mood', 1216),\n ('smile', 1213),\n ('fast', 1212),\n ('short', 1211),\n ('turn', 1207),\n ('awards', 1206),\n ('supposed', 1201),\n ('tickets', 1200),\n ('ride', 1193),\n ('unfortunately', 1193),\n ('list', 1192),\n ('hmm', 1190),\n ('broken', 1190),\n ('blip', 1188),\n ('black', 1186),\n ('air', 1184),\n ('myspace', 1181),\n ('writing', 1179),\n ('word', 1178),\n ('moving', 1177),\n ('worst', 1176),\n ('saying', 1161),\n ('followfriday', 1159),\n ('da', 1147),\n ('chocolate', 1147),\n ('understand', 1146),\n ('yep', 1143),\n ('pictures', 1141),\n ('uk', 1140),\n ('horrible', 1136),\n ('worth', 1135),\n ('throat', 1133),\n ('sent', 1132),\n ('star', 1130),\n ('driving', 1129),\n ('london', 1126),\n ('three', 1125),\n ('pick', 1124),\n ('photo', 1124),\n ('wonder', 1115),\n ('park', 1113),\n ('sleepy', 1112),\n ('past', 1111),\n ('dreams', 1111),\n ('gave', 1109),\n ('sunshine', 1106),\n ('via', 1105),\n ('cleaning', 1102),\n ('account', 1100),\n ('em', 1099),\n ('college', 1095),\n ('xxx', 1086),\n ('sat', 1085),\n ('slow', 1084),\n ('wedding', 1083),\n ('team', 1081),\n ('huge', 1080),\n ('fell', 1079),\n ('forget', 1079),\n ('drinking', 1078),\n ('mum', 1075),\n ('jonas', 1073),\n ('plan', 1066),\n ('mr', 1064),\n ('wouldn', 1063),\n ('lady', 1063),\n ('whats', 1061),\n ('tweeting', 1059),\n ('rather', 1055),\n ('worse', 1047),\n ('miley', 1047),\n ('upset', 1044),\n ('green', 1044),\n ('mac', 1043),\n ('chance', 1043),\n ('under', 1043),\n ('easy', 1042),\n ('updates', 1042),\n ('doesnt', 1039),\n ('moon', 1038),\n ('special', 1036),\n ('tuesday', 1034),\n ('date', 1030),\n ('band', 1029),\n ('hand', 1028),\n ('mtv', 1026),\n ('flight', 1025),\n ('aren', 1023),\n ('forever', 1023),\n ('holiday', 1023),\n ('apparently', 1021),\n ('spent', 1019),\n ('scared', 1016),\n ('point', 1012),\n ('parents', 1012),\n ('flu', 1010),\n ('bday', 1010),\n ('vote', 1009),\n ('nope', 1009),\n ('due', 1003),\n ('body', 1001),\n ('bet', 1000),\n ('longer', 998),\n ('slept', 996),\n ('plans', 995),\n ('hugs', 994),\n ('plus', 993),\n ('spend', 992),\n ('thx', 989),\n ('wondering', 987),\n ('son', 985),\n ('wtf', 982),\n ('lazy', 981),\n ('line', 980),\n ('lets', 980),\n ('ahhh', 977),\n ('photos', 976),\n ('fair', 974),\n ('worry', 973),\n ('cannot', 972),\n ('hanging', 972),\n ('website', 970),\n ('thursday', 970),\n ('white', 964),\n ('especially', 964),\n ('join', 962),\n ('voice', 960),\n ('shows', 959),\n ('during', 958),\n ('chat', 957),\n ('answer', 957),\n ('vacation', 957),\n ('cake', 953),\n ('idk', 952),\n ('message', 951),\n ('shame', 950),\n ('bus', 949),\n ('google', 948),\n ('different', 946),\n ('earlier', 945),\n ('bbq', 940),\n ('warm', 939),\n ('wear', 938),\n ('july', 938),\n ('beer', 936),\n ('david', 933),\n ('crying', 930),\n ('tom', 929),\n ('episode', 929),\n ('havent', 926),\n ('stomach', 924),\n ('fans', 923),\n ('learn', 922),\n ('looked', 918),\n ('support', 916),\n ('words', 916),\n ('sims', 916),\n ('camera', 915),\n ('safe', 914),\n ('luv', 913),\n ('yummy', 911),\n ('dm', 911),\n ('sadly', 911),\n ('pizza', 909),\n ('radio', 902),\n ('fix', 901),\n ('die', 900),\n ('thinks', 896),\n ('hrs', 895),\n ('except', 892),\n ('project', 892),\n ('ff', 890),\n ('figure', 888),\n ('rainy', 885),\n ('shall', 884),\n ('hubby', 883),\n ('garden', 883),\n ('laugh', 881),\n ('airport', 881),\n ('met', 880),\n ('tummy', 879),\n ('cd', 877),\n ('meant', 877),\n ('books', 877),\n ('mins', 873),\n ('gorgeous', 872),\n ('yourself', 872),\n ('bike', 871),\n ('graduation', 871),\n ('officially', 870),\n ('dress', 869),\n ('apple', 869),\n ('boyfriend', 868),\n ('shirt', 868),\n ('liked', 868),\n ('worked', 867),\n ('finals', 867),\n ('paper', 866),\n ('hospital', 865),\n ('ps', 862),\n ('eye', 862),\n ('inside', 859),\n ('decided', 857),\n ('beat', 856),\n ('road', 855),\n ('small', 855),\n ('blue', 854),\n ('number', 853),\n ('babe', 853),\n ('games', 852),\n ('case', 852),\n ('power', 852),\n ('shoes', 852),\n ('shop', 847),\n ('each', 845),\n ('save', 844),\n ('hug', 844),\n ('ouch', 844),\n ('yo', 836),\n ('lame', 834),\n ('wishes', 834),\n ('annoying', 831),\n ('club', 829),\n ('yup', 827),\n ('chicken', 826),\n ('wit', 826),\n ('sign', 822),\n ('brothers', 821),\n ('hmmm', 820),\n ('proud', 819),\n ('cos', 819),\n ('behind', 816),\n ('kid', 815),\n ('felt', 815),\n ('card', 812),\n ('alright', 811),\n ('played', 811),\n ('exciting', 810),\n ('feet', 810),\n ('taken', 808),\n ('front', 807),\n ('hates', 806),\n ('xoxo', 804),\n ('living', 803),\n ('wednesday', 800),\n ('woo', 800),\n ('service', 799),\n ('exactly', 798),\n ('starts', 797),\n ('although', 795),\n ('bro', 795),\n ('lonely', 795),\n ('kill', 793),\n ('french', 793),\n ('mothers', 789),\n ('everybody', 789),\n ('goodbye', 788),\n ('knows', 786),\n ('absolutely', 785),\n ('business', 784),\n ('share', 784),\n ('needed', 783),\n ('packing', 782),\n ('twilight', 781),\n ('turned', 781),\n ('ma', 779),\n ('whatever', 777),\n ('download', 774),\n ('waking', 772),\n ('jus', 771),\n ('scary', 771),\n ('order', 770),\n ('minute', 768),\n ('isnt', 768),\n ('wine', 766),\n ('gr', 765),\n ('eh', 765),\n ('relaxing', 763),\n ('yum', 763),\n ('lose', 763),\n ('pass', 761),\n ('near', 760),\n ('fact', 758),\n ('realized', 758),\n ('gd', 757),\n ('giving', 754),\n ('question', 754),\n ('along', 753),\n ('mail', 752),\n ('vegas', 750),\n ('clothes', 749),\n ('keeps', 749),\n ('happens', 749),\n ('guitar', 748),\n ('walking', 748),\n ('yours', 747),\n ('app', 747),\n ('sit', 746),\n ('touch', 745),\n ('cup', 744),\n ('profile', 742),\n ('killing', 742),\n ('sold', 742),\n ('posted', 739),\n ('drunk', 736),\n ('wife', 735),\n ('gettin', 732),\n ('film', 730),\n ('alot', 729),\n ('sis', 725),\n ('fantastic', 722),\n ('interview', 721),\n ('completely', 717),\n ('ohh', 716),\n ('box', 715),\n ('history', 713),\n ('revision', 710),\n ('argh', 709),\n ('fly', 707),\n ('version', 706),\n ('bb', 706),\n ('lakers', 702),\n ('staying', 701),\n ('asked', 701),\n ...]"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove words which occur less than O times\n",
    "O = 10\n",
    "    \n",
    "training_text = (' '.join(df['text'])).split()\n",
    "test_text = (' '.join(df_test['text'])).split()\n",
    "dataset_counter = Counter(training_text + test_text)\n",
    "\n",
    "dataset_counter.most_common()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-28T19:35:06.582478900Z",
     "start_time": "2023-07-28T19:35:05.105603700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/500000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c3536b708d5c4e0182460469a61f2730"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "        target                                               text\n0            0  this is my last tweet of the day so goodnight ...\n1            1  laughing okay yeah fer suree ll vote for her r...\n2            0  gonna feel like shit at uni today still whacke...\n3            0  can find my phone charger so switching my serv...\n4            0                   it was just fade to black though\n...        ...                                                ...\n499995       1                          watching twilight text me\n499996       1  for me it is usual day for me fighting with my...\n499997       0  question for you can stress re awaken ptl yest...\n499998       0                              okay then made me sad\n499999       0  what about with me now sad and you can say any...\n\n[500000 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>this is my last tweet of the day so goodnight ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>laughing okay yeah fer suree ll vote for her r...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>gonna feel like shit at uni today still whacke...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>can find my phone charger so switching my serv...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>it was just fade to black though</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>499995</th>\n      <td>1</td>\n      <td>watching twilight text me</td>\n    </tr>\n    <tr>\n      <th>499996</th>\n      <td>1</td>\n      <td>for me it is usual day for me fighting with my...</td>\n    </tr>\n    <tr>\n      <th>499997</th>\n      <td>0</td>\n      <td>question for you can stress re awaken ptl yest...</td>\n    </tr>\n    <tr>\n      <th>499998</th>\n      <td>0</td>\n      <td>okay then made me sad</td>\n    </tr>\n    <tr>\n      <th>499999</th>\n      <td>0</td>\n      <td>what about with me now sad and you can say any...</td>\n    </tr>\n  </tbody>\n</table>\n<p>500000 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_infrequent_words(sentence):\n",
    "    _words = sentence.split()\n",
    "    _words = [word if dataset_counter[word] >= O else ' ' for word in _words]\n",
    "    \n",
    "    return ' '.join(_words)\n",
    "    \n",
    "df['text'] = df['text'].progress_apply(remove_infrequent_words)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-28T19:35:17.392256900Z",
     "start_time": "2023-07-28T19:35:15.723737800Z"
    }
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "# read cleaned data\n",
    "df = pd.read_csv('data_cleaned.csv')\n",
    "df_test = pd.read_csv('data_valid_cleaned.csv')\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "# read raw data\n",
    "df = pd.read_csv('data.csv')\n",
    "df_test = pd.read_csv('data_valid.csv')\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "        target                                               text\n0            0  this is my last tweet of the day so goodnight ...\n1            1  laughing okay yeah fer suree ll vote for her r...\n2            0  gonna feel like shit at uni today still whacke...\n3            0  can find my phone charger so switching my serv...\n4            0                   it was just fade to black though\n...        ...                                                ...\n499995       1                          watching twilight text me\n499996       1  for me it is usual day for me fighting with my...\n499997       0  question for you can stress re awaken ptl yest...\n499998       0                              okay then made me sad\n499999       0  what about with me now sad and you can say any...\n\n[500000 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>this is my last tweet of the day so goodnight ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>laughing okay yeah fer suree ll vote for her r...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>gonna feel like shit at uni today still whacke...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>can find my phone charger so switching my serv...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>it was just fade to black though</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>499995</th>\n      <td>1</td>\n      <td>watching twilight text me</td>\n    </tr>\n    <tr>\n      <th>499996</th>\n      <td>1</td>\n      <td>for me it is usual day for me fighting with my...</td>\n    </tr>\n    <tr>\n      <th>499997</th>\n      <td>0</td>\n      <td>question for you can stress re awaken ptl yest...</td>\n    </tr>\n    <tr>\n      <th>499998</th>\n      <td>0</td>\n      <td>okay then made me sad</td>\n    </tr>\n    <tr>\n      <th>499999</th>\n      <td>0</td>\n      <td>what about with me now sad and you can say any...</td>\n    </tr>\n  </tbody>\n</table>\n<p>500000 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the csv file\n",
    "df = pd.read_csv('data_cleaned.csv')\n",
    "df_test = pd.read_csv('data_valid_cleaned.csv')\n",
    "\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-28T19:10:56.759415400Z",
     "start_time": "2023-07-28T19:10:56.065180200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "('sizes:', (500000,), (500000,), (100000,))"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def drop_not_string(_df, column):\n",
    "#     return _df.drop(_df[_df[column].apply(lambda x: isinstance(x, str)) == False].index)\n",
    "# \n",
    "# drop_not_string(df, 'text')\n",
    "# drop_not_string(df_test, 'text')\n",
    "\n",
    "X = df['text']\n",
    "y = df['target']\n",
    "\n",
    "X_test = df_test['text']\n",
    "\n",
    "# make them all strings\n",
    "X = X.astype(str)\n",
    "X_test = X_test.astype(str)\n",
    "\n",
    "\"sizes:\", X.shape, y.shape, X_test.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-28T19:10:56.868618800Z",
     "start_time": "2023-07-28T19:10:56.747415300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word2Vec\n",
    "Using the Gensim implementation of Google's Word2Vec."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/500000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bae608eff1924134bca59f4c2e438abd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/100000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "de44893d97b942ffbfccf17b723c3842"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "(500000, 100)"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = [sentence.split() for sentence in X]\n",
    "w2v_model = Word2Vec(sentences, window=5, min_count=5, workers=4, hs=1 , negative=0)\n",
    "\n",
    "def vectorize(sentence):\n",
    "    words = sentence.split()\n",
    "    words_vecs = [w2v_model.wv[word] for word in words if word in w2v_model.wv]\n",
    "    if len(words_vecs) == 0:\n",
    "        return np.zeros(100)\n",
    "    words_vecs = np.array(words_vecs)\n",
    "    return words_vecs.mean(axis=0)\n",
    "\n",
    "X_w2v = np.array([vectorize(sentence) for sentence in tqdm(X)])\n",
    "X_test_w2v = np.array([vectorize(sentence) for sentence in tqdm(X_test)])\n",
    "\n",
    "X_w2v.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-28T19:11:30.911865400Z",
     "start_time": "2023-07-28T19:10:56.787097700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Skip-Gram"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "from toolz import itertoolz, compose\n",
    "from toolz.curried import map as cmap, sliding_window, pluck\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "class SkipGramVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):    \n",
    "        preprocess = self.build_preprocessor()\n",
    "        stop_words = self.get_stop_words()\n",
    "        tokenize = self.build_tokenizer()\n",
    "        return lambda doc: self._word_skip_grams(\n",
    "                compose(tokenize, preprocess, self.decode)(doc),\n",
    "                stop_words)\n",
    "\n",
    "    def _word_skip_grams(self, tokens, stop_words=None):\n",
    "        # handle stop words\n",
    "        if stop_words is not None:\n",
    "            tokens = [w for w in tokens if w not in stop_words]\n",
    "\n",
    "        return compose(cmap(' '.join), pluck([0, 2]), sliding_window(3))(tokens)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Mapping Visualisation\n",
    "The 100-dimensional Word2Vec mapping of our features is reduced to a 2-dimensional space. The colouring is according to the label."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "import umap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "reducer = umap.UMAP()\n",
    "scaled_we = StandardScaler().fit_transform(X_w2v)\n",
    "_embedding1 = reducer.fit_transform(scaled_we[:N])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(\n",
    "    _embedding1[:, 0],\n",
    "    _embedding1[:, 1],\n",
    "    color=['r' if _y == 0 else 'b' for _y in y[:N]]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classifiers\n",
    "This compares different classifiers. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "35ba1769863a4fdf90a146bc0350d41d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  embedding            classifier  train accuracy  test accuracy\n0       w2v  LogisticRegression()        0.751424       0.750212\n1       w2v     RidgeClassifier()        0.747737       0.746988\n2       w2v          GaussianNB()        0.654101       0.655412",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>embedding</th>\n      <th>classifier</th>\n      <th>train accuracy</th>\n      <th>test accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>w2v</td>\n      <td>LogisticRegression()</td>\n      <td>0.751424</td>\n      <td>0.750212</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>w2v</td>\n      <td>RidgeClassifier()</td>\n      <td>0.747737</td>\n      <td>0.746988</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>w2v</td>\n      <td>GaussianNB()</td>\n      <td>0.654101</td>\n      <td>0.655412</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "embeddings = [{\"name\": \"w2v\", \"data\": X_w2v}, ]\n",
    "classifiers = [LogisticRegression(), RidgeClassifier(), GaussianNB()]\n",
    "\n",
    "for e in embeddings:\n",
    "    _X_train, _X_test, _y_train, _y_test = train_test_split(e[\"data\"], y, test_size=0.33)\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    \n",
    "    for c in tqdm(classifiers):\n",
    "        c.fit(_X_train, _y_train)\n",
    "        _y_hat_train = c.predict(_X_train)\n",
    "        _y_hat_test = c.predict(_X_test)\n",
    "        \n",
    "        train_acc.append(accuracy_score(_y_hat_train, _y_train))\n",
    "        test_acc.append(accuracy_score(_y_hat_test, _y_test))\n",
    "        \n",
    "pd.DataFrame({'embedding': list(map(lambda e: e[\"name\"], embeddings))*len(classifiers), 'classifier': classifiers, 'train accuracy': train_acc, 'test accuracy': test_acc})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-28T19:11:35.401891300Z",
     "start_time": "2023-07-28T19:11:30.904864500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save prediction with Logistic Regression to file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_w2v, y)\n",
    "y_pred = lr.predict(X_w2v[:N])\n",
    "accuracy_score(y[:N], y_pred)\n",
    "\n",
    "y_pred_test = lr.predict(X_test_w2v)\n",
    "np.save('y_pred.npy', y_pred_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-28T19:17:36.880820Z",
     "start_time": "2023-07-28T19:17:33.628137300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transformer\n",
    "Using a pre-trained transformer for sentiment analysis from Huggingface.\n",
    "Score on the validation set: 0.71\n",
    "\n",
    "This method is therefore worse than what I used before..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "from transformers import pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "y_pred_transformer = sentiment_pipeline(df_test['text'].tolist())\n",
    "\n",
    "y_pred_transformer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "# np.array([1 if x['label'] == 'POSITIVE' else 0 for x in y_pred_transformer]).tofile('y_pred_transformer.npy')\n",
    "y_pred_transformer_array = np.array(y_pred_transformer)\n",
    "np.save('y_pred_transformer.npy', y_pred_transformer_array)\n",
    "np.array(y_pred_transformer).shape[0] == len(df_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "# type(df['text'].tolist())\n",
    "_x = np.load(\"y_pred_transformer.npy\", allow_pickle=True)\n",
    "np.all(_x == y_pred_transformer_array)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
