{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# AGDML-Lab Final\n",
    "Task: Sentiment Analysis of Twitter messages\n",
    "\n",
    "## Preprocessing\n",
    "For preprocessing I used NLTK library. I removed non-alphabetic characters, made words lowercase, removed mentions of other users, removed stopwords and lemmatized each word to its lemma. This should make the data more consistent and easier to work with. My assumption is: most of the spelling mistakes and special characters are unnecessary for sentiment analysis.\n",
    "\n",
    "The regex will most likely match stuff that we do not want removed, but that is a tradeoff we accept."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-28T08:57:13.001612200Z",
     "start_time": "2023-07-28T08:57:12.947872200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/500000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "094d3738719b4da5a042b5bec011363b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/100000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "297ebf0bbe8541299b4de516b1498e7b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# read training data\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# read validation data\n",
    "df_test = pd.read_csv('data_valid.csv')\n",
    "\n",
    "# preprocessing text messages\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# download stopwords and wordnet\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# create object of WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# function to clean text data\n",
    "def clean_text(text):\n",
    "    # make words lowercase, because Go and go will be considered as two words\n",
    "    text = text.lower()\n",
    "    # remove URLs from text\n",
    "    text = re.sub('(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)', ' ', text)\n",
    "    # remove mentions of other users\n",
    "    text = re.sub('\\B@[._a-z0-9]{3,24}', '', text)\n",
    "    # replace ampersand html tag with &\n",
    "    text = re.sub('\\&amp;', 'and', text)\n",
    "    # remove very special characters\n",
    "    # text = re.sub('[^A-Za-z!?\\.\\-]', ' ', text)\n",
    "    # split the sentences into words\n",
    "    words = text.split()\n",
    "    # remove stopwords like to, and, or etc.\n",
    "    words = [word for word in words if word not in set(stopwords.words('english'))]\n",
    "    # lemmatize each word to its lemma\n",
    "    words = [wordnet_lemmatizer.lemmatize(word) for word in words]\n",
    "    # join words to make sentence\n",
    "    text = ' '.join(words)\n",
    "    return text\n",
    "\n",
    "# drop rows with missing values\n",
    "df = df.dropna()\n",
    "df_test = df_test.dropna()\n",
    "\n",
    "# clean text data\n",
    "df['text'] = df['text'].progress_apply(clean_text)\n",
    "df.to_csv('data_cleaned.csv', index=False)\n",
    "\n",
    "df_test['text'] = df_test['text'].progress_apply(clean_text)\n",
    "df_test.to_csv('data_valid_cleaned.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-28T08:57:24.989754400Z",
     "start_time": "2023-07-28T08:57:12.954613100Z"
    }
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "# read cleaned data\n",
    "df = pd.read_csv('data_cleaned.csv')\n",
    "df_test = pd.read_csv('data_valid_cleaned.csv')\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "# read raw data\n",
    "df = pd.read_csv('data.csv')\n",
    "df_test = pd.read_csv('data_valid.csv')\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [
    {
     "data": {
      "text/plain": "Empty DataFrame\nColumns: [target, text]\nIndex: []",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['text'].apply(lambda x: not isinstance(x, str))]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-28T08:57:25.065543Z",
     "start_time": "2023-07-28T08:57:24.990758Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "y = df['target']\n",
    "\n",
    "X_test = df_test['text']\n",
    "\n",
    "# # assure that it's all text\n",
    "# X = [str(x) for x in X]\n",
    "# X_test = [str(x) for x in X_test]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-28T08:57:25.080232100Z",
     "start_time": "2023-07-28T08:57:25.066543200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/500000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c7eef426bb9c41d993f939c28acafcc4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/100000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d753a65b3880490c8d19b4137e35ce80"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = [sentence.split() for sentence in X]\n",
    "w2v_model = Word2Vec(sentences, window=5, min_count=5, workers=4, hs=1 , negative=0)\n",
    "\n",
    "def vectorize(sentence):\n",
    "    words = sentence.split()\n",
    "    words_vecs = [w2v_model.wv[word] for word in words if word in w2v_model.wv]\n",
    "    if len(words_vecs) == 0:\n",
    "        return np.zeros(100)\n",
    "    words_vecs = np.array(words_vecs)\n",
    "    return words_vecs.mean(axis=0)\n",
    "\n",
    "X_w2v = np.array([vectorize(sentence) for sentence in tqdm(X)])\n",
    "X_test_w2v = np.array([vectorize(sentence) for sentence in tqdm(X_test)])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-28T08:58:03.438413500Z",
     "start_time": "2023-07-28T08:57:25.083230200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xe0 in position 3: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m                        Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[150], line 214\u001B[0m\n\u001B[0;32m    210\u001B[0m         plt\u001B[38;5;241m.\u001B[39msavefig(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLDA_tSNE_\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.png\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(time\u001B[38;5;241m.\u001B[39mstrftime(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mY\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mm\u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mH\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mM\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mS\u001B[39m\u001B[38;5;124m\"\u001B[39m)), bbox_inches\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtight\u001B[39m\u001B[38;5;124m'\u001B[39m, dpi\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m500\u001B[39m)\n\u001B[0;32m    212\u001B[0m     plt\u001B[38;5;241m.\u001B[39mshow()\n\u001B[1;32m--> 214\u001B[0m \u001B[43mLDA_tSNE_topics_vis\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcorpus\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX_w2v\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[150], line 44\u001B[0m, in \u001B[0;36mLDA_tSNE_topics_vis\u001B[1;34m(dimension, corpus, num_topics, remove_3d_outliers, save_png)\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mLDA_tSNE_topics_vis\u001B[39m(dimension\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mboth\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     13\u001B[0m                         corpus\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \n\u001B[0;32m     14\u001B[0m                         num_topics\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m,\n\u001B[0;32m     15\u001B[0m                         remove_3d_outliers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m     16\u001B[0m                         save_png\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m     17\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;124;03m    Returns the outputs of an LDA model plotted using t-SNE (t-distributed Stochastic Neighbor Embedding)\u001B[39;00m\n\u001B[0;32m     19\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     42\u001B[0m \u001B[38;5;124;03m        A t-SNE lower dimensional representation of an LDA model's topics and their constituent members\u001B[39;00m\n\u001B[0;32m     43\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 44\u001B[0m     dirichlet_dict \u001B[38;5;241m=\u001B[39m \u001B[43mcorpora\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDictionary\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcorpus\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     45\u001B[0m     bow_corpus \u001B[38;5;241m=\u001B[39m [dirichlet_dict\u001B[38;5;241m.\u001B[39mdoc2bow(text) \u001B[38;5;28;01mfor\u001B[39;00m text \u001B[38;5;129;01min\u001B[39;00m corpus]\n\u001B[0;32m     47\u001B[0m     dirichlet_model \u001B[38;5;241m=\u001B[39m LdaModel(corpus\u001B[38;5;241m=\u001B[39mbow_corpus,\n\u001B[0;32m     48\u001B[0m                                id2word\u001B[38;5;241m=\u001B[39mdirichlet_dict,\n\u001B[0;32m     49\u001B[0m                                num_topics\u001B[38;5;241m=\u001B[39mnum_topics,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     53\u001B[0m                                alpha\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     54\u001B[0m                                random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m) \u001B[38;5;66;03m# set for testing\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gensim\\corpora\\dictionary.py:78\u001B[0m, in \u001B[0;36mDictionary.__init__\u001B[1;34m(self, documents, prune_at)\u001B[0m\n\u001B[0;32m     75\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_nnz \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     77\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m documents \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 78\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_documents\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdocuments\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprune_at\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprune_at\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     79\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madd_lifecycle_event(\n\u001B[0;32m     80\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcreated\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     81\u001B[0m         msg\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbuilt \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m from \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_docs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m documents (total \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_pos\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m corpus positions)\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     82\u001B[0m     )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gensim\\corpora\\dictionary.py:204\u001B[0m, in \u001B[0;36mDictionary.add_documents\u001B[1;34m(self, documents, prune_at)\u001B[0m\n\u001B[0;32m    201\u001B[0m         logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124madding document #\u001B[39m\u001B[38;5;132;01m%i\u001B[39;00m\u001B[38;5;124m to \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, docno, \u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m    203\u001B[0m     \u001B[38;5;66;03m# update Dictionary with the document\u001B[39;00m\n\u001B[1;32m--> 204\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdoc2bow\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdocument\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallow_update\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# ignore the result, here we only care about updating token ids\u001B[39;00m\n\u001B[0;32m    206\u001B[0m logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbuilt \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m from \u001B[39m\u001B[38;5;132;01m%i\u001B[39;00m\u001B[38;5;124m documents (total \u001B[39m\u001B[38;5;132;01m%i\u001B[39;00m\u001B[38;5;124m corpus positions)\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_docs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_pos)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gensim\\corpora\\dictionary.py:246\u001B[0m, in \u001B[0;36mDictionary.doc2bow\u001B[1;34m(self, document, allow_update, return_missing)\u001B[0m\n\u001B[0;32m    244\u001B[0m counter \u001B[38;5;241m=\u001B[39m defaultdict(\u001B[38;5;28mint\u001B[39m)\n\u001B[0;32m    245\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m w \u001B[38;5;129;01min\u001B[39;00m document:\n\u001B[1;32m--> 246\u001B[0m     counter[w \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(w, \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mw\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mutf-8\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    248\u001B[0m token2id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtoken2id\n\u001B[0;32m    249\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m allow_update \u001B[38;5;129;01mor\u001B[39;00m return_missing:\n",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m: 'utf-8' codec can't decode byte 0xe0 in position 3: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "from datetime import time\n",
    "from matplotlib.pyplot import plot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "\n",
    "from gensim.models import LdaModel\n",
    "from gensim import corpora\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "light_grey_tup=(0.9, 0.9, 0.9)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-28T09:04:11.637802200Z",
     "start_time": "2023-07-28T09:04:11.468120800Z"
    }
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "def predict(X : np.ndarray, theta: np.ndarray) -> np.ndarray:\n",
    "    '''\n",
    "    Predicts Y given X and theta.\n",
    "    \n",
    "    @Params:\n",
    "        X... matrix with datapoints as rows (m x n)\n",
    "        theta... parameter vector (n)\n",
    "        \n",
    "    @Returns:\n",
    "        array of predictions (m)\n",
    "    '''\n",
    "    # implement\n",
    "    return np.where(X @ theta >= 0, 1, 0)\n",
    "\n",
    "def sigmoid(X : np.ndarray) -> np.ndarray:\n",
    "    '''\n",
    "    Sigmoid function.\n",
    "    \n",
    "    @Params:\n",
    "        X... array of x\n",
    "        \n",
    "    @Returns:\n",
    "        array of sigmoid(x)\n",
    "    '''\n",
    "    # implement\n",
    "    return 1 / (1 + np.exp(-1 * X))\n",
    "\n",
    "def acc(X : np.ndarray, Y : np.ndarray, theta : np.ndarray) -> float:\n",
    "    '''\n",
    "    Accuracy.\n",
    "    \n",
    "    @Params:\n",
    "        X... matrix with datapoints as rows (m x n)\n",
    "        Y... array of true labels (n)\n",
    "        theta... parameter vector (n)\n",
    "        \n",
    "    @Returns:\n",
    "        binary cross entropy\n",
    "    '''\n",
    "    \n",
    "    # implement\n",
    "    labels = Y.flatten()\n",
    "    predictions = predict(X, theta).flatten()\n",
    "\n",
    "    return np.mean(list(map(lambda x_i, y_i: x_i*y_i + (1 - x_i)*(1 - y_i), labels, predictions))).__float__()\n",
    "\n",
    "def cross_entropy_loss(X : np.ndarray, Y : np.ndarray, theta : np.ndarray) -> float:\n",
    "    '''\n",
    "    Binary cross entropy loss.\n",
    "    \n",
    "    @Params:\n",
    "        X... matrix with datapoints as rows (m x n)\n",
    "        Y... array of true labels (n)\n",
    "        theta... parameter vector (n)\n",
    "        \n",
    "    @Returns:\n",
    "        binary cross entropy\n",
    "    '''\n",
    "    # implement\n",
    "\n",
    "    p = sigmoid(X @ theta)\n",
    "    return -1 * np.mean(Y * np.log(p) + (1-Y) * np.log(1-p))\n",
    "\n",
    "def cross_entropy_gradient(X : np.ndarray, Y : np.ndarray, theta : np.ndarray):\n",
    "    '''\n",
    "    Gradient of binary crossentropy. wrt theta.\n",
    "    \n",
    "    @Params:\n",
    "        X... matrix with datapoints as rows (m x n)\n",
    "        Y... array of true labels (n)\n",
    "        theta... parameter vector (n)\n",
    "        \n",
    "    @Returns:\n",
    "        gradient at point theta\n",
    "    '''\n",
    "    # implement\n",
    "    return 1/X.shape[0] * (X.T @ (sigmoid(X @ theta) - Y))\n",
    "\n",
    "class LogReg():\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Class for Logistic regression.\n",
    "        '''\n",
    "        self.theta = None # parameter vector\n",
    "        self.accs = [] # accuracies\n",
    "        self.losses = [] # losses\n",
    "\n",
    "\n",
    "    def predict(self, X : np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        Predicts Y given X and learned theta.\n",
    "\n",
    "        @Params:\n",
    "            X... matrix with datapoints as rows (m x n)\n",
    "\n",
    "        @Returns:\n",
    "            array of predictions (m)\n",
    "        '''\n",
    "        return predict(X, self.theta)\n",
    "    \n",
    "    \n",
    "    def fit(self, X : np.ndarray, Y : np.ndarray, lr : float = 1e-2, max_it : int = 1000, eps : float = 1e-5) -> None:\n",
    "        '''\n",
    "        Gradient descend for binary crossentropy.\n",
    "\n",
    "        @Params:\n",
    "            X... matrix with datapoints as rows (m x n)\n",
    "            Y... array of true labels (n)\n",
    "            lr... learnrate, sets stepsize for descend\n",
    "            max_it... maximum number of steps\n",
    "            eps... abort criterium for early stopping (loss did not change more than this)\n",
    "\n",
    "        '''\n",
    "\n",
    "        self.theta = np.random.rand(X.shape[1])\n",
    "        print(\"theta\", self.theta.shape)\n",
    "\n",
    "        for i in range(max_it):\n",
    "            loss = cross_entropy_loss(X, Y, self.theta)\n",
    "            accuracy = acc(X, Y, self.theta)\n",
    "\n",
    "            self.losses.append(loss)\n",
    "            self.accs.append(accuracy)\n",
    "\n",
    "            ceg = cross_entropy_gradient(X, Y, self.theta)\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Iter {i}, Accuracy: {accuracy}\")\n",
    "\n",
    "            if loss < eps:\n",
    "                break\n",
    "\n",
    "            self.theta -= ceg\n",
    "\n",
    "        print(\"\\nAccuracy start:\", self.accs[0], \"\\nAccuracy finish:\", self.accs[-1])\n",
    "        print(\"Finished due to\", \"max_it reached\" if i == max_it-1 else \"change smaller than epsilon\")\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [],
   "source": [
    "# use sklearn to learn a logistic regression classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "classifier = LogisticRegression(max_iter=1000, C=1e5)\n",
    "classifier.fit(X_w2v, y)\n",
    "\n",
    "# predict the test set results\n",
    "y_pred = classifier.predict(X_test_w2v)\n",
    "\n",
    "# save the results to npy\n",
    "df_test['target'] = y_pred\n",
    "np.save('y_pred.npy', y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-28T08:58:06.581543200Z",
     "start_time": "2023-07-28T08:58:03.456377100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy:  0.726026\n",
      "standard deviation:  0.002057300172556244\n"
     ]
    }
   ],
   "source": [
    "# make k-fold cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator=classifier, X=X_w2v, y=y, cv=10)\n",
    "print('mean accuracy: ', accuracies.mean())\n",
    "print('standard deviation: ', accuracies.std())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-28T08:58:35.698986900Z",
     "start_time": "2023-07-28T08:58:06.582545300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transformer\n",
    "Using a pre-trained transformer for sentiment analysis from Huggingface.\n",
    "Score on the validation set: 0.71\n",
    "\n",
    "This method is therefore worse than what I used before..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "from transformers import pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "y_pred_transformer = sentiment_pipeline(df_test['text'].tolist())\n",
    "\n",
    "y_pred_transformer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "# np.array([1 if x['label'] == 'POSITIVE' else 0 for x in y_pred_transformer]).tofile('y_pred_transformer.npy')\n",
    "y_pred_transformer_array = np.array(y_pred_transformer)\n",
    "np.save('y_pred_transformer.npy', y_pred_transformer_array)\n",
    "np.array(y_pred_transformer).shape[0] == len(df_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "# type(df['text'].tolist())\n",
    "_x = np.load(\"y_pred_transformer.npy\", allow_pickle=True)\n",
    "np.all(_x == y_pred_transformer_array)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
