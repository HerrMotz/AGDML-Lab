{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# AGDML-Lab Final\n",
    "Task: Sentiment Analysis of Twitter messages\n",
    "\n",
    "## Preprocessing\n",
    "For preprocessing I used NLTK library. I removed non-alphabetic characters, made words lowercase, removed mentions of other users, removed stopwords and lemmatized each word to its lemma. This should make the data more consistent and easier to work with. My assumption is: most of the spelling mistakes and special characters are unnecessary for sentiment analysis.\n",
    "\n",
    "The regex will most likely match stuff that we do not want removed, but that is a tradeoff we accept."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "'NVIDIA GeForce GTX 1060 6GB'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "tqdm.pandas()\n",
    "torch.cuda.get_device_name(0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-28T12:19:29.779901Z",
     "start_time": "2023-07-28T12:19:28.097333300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wouldn', 'any', 'off', \"isn't\", \"mightn't\", 's', 'until', \"it's\", 'only', 'yourselves', 'ourselves', \"shouldn't\", 'why', 'between', 'before', 'not', 'while', 'at', 'below', 'isn', 'yourself', \"needn't\", 'most', \"weren't\", 'some', 'were', 'too', 'other', 'their', 'theirs', 'during', 'had', 'hers', 'here', 'shouldn', \"wouldn't\", 'my', 'once', 'against', 'needn', 'mustn', \"aren't\", \"should've\", 'then', 'own', 'she', 'they', \"hadn't\", 'further', 'did', \"that'll\", \"you're\", 'what', 'about', 'weren', 'than', 'by', 'because', 'out', 'will', 'a', 'if', 'can', 't', 'in', 'as', 'just', 'doing', 'same', \"you'll\", 'hasn', 'no', \"couldn't\", 'few', 'll', 'but', 'should', \"haven't\", 'have', 'there', 'shan', 'having', 'above', 'and', \"won't\", 'ain', 'into', 'you', 'under', \"she's\", 'be', 'won', 'from', 'these', 'wasn', 're', 'o', 'd', 'couldn', 'herself', \"you've\", 'for', 'your', 'doesn', 'mightn', 'after', 'whom', 'myself', 'itself', 'aren', 'ours', 'so', 'over', 'was', 'its', 'been', 'has', 'the', 'each', 'are', 'himself', 'those', 'ma', 'both', 'now', 'who', 'when', \"mustn't\", \"didn't\", 'to', 'hadn', 'themselves', 'nor', 'him', 'how', 'all', 'an', 'up', 'that', 'y', 'does', 'where', 'it', 'or', 'through', \"you'd\", 'down', 'do', 'yours', 'on', \"hasn't\", 'his', 'her', 'with', 'such', \"don't\", \"doesn't\", \"wasn't\", 'our', 'being', 'i', 'of', 'didn', 've', \"shan't\", 'm', 'again', 'haven', 'them', 'this', 'is', 'am', 'he', 'we', 'me', 'more', 'which', 'don'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\frand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\frand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/500000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1ca4fad132fb4ed3aa378b848a92edfe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/100000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fdecf20ab1124e3080cb9c5266a57f7c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# read training data\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# read validation data\n",
    "df_test = pd.read_csv('data_valid.csv')\n",
    "\n",
    "# preprocessing text messages\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# download stopwords and wordnet\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# create object of WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# this allows very\n",
    "stopwords = set(stopwords.words('english'))\n",
    "stopwords.remove('very')\n",
    "print(stopwords)\n",
    "\n",
    "# function to clean text data\n",
    "def clean_text(text):\n",
    "    # remove mentions of other users\n",
    "    text = re.sub('\\B@[._a-zA-Z0-9]{3,24}', '', text)\n",
    "    # rewrite words in all caps to \"very\" followed by word\n",
    "    text = re.sub('([A-Z]+)', lambda x: 'very ' + x.group(0).lower(), text)\n",
    "    # make words lowercase, because Go and go will be considered as two words\n",
    "    text = text.lower()\n",
    "    # remove URLs from text\n",
    "    text = re.sub('(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)', ' ', text)\n",
    "    # replace ampersand html tag with &\n",
    "    text = re.sub('\\&amp;', 'and', text)\n",
    "    # replace text surrounded by ** with very\n",
    "    text = re.sub('\\*([a-z])\\*', r'very \\1', text)\n",
    "    # remove everything but letters\n",
    "    text = re.sub('[^a-z]', ' ', text)    \n",
    "    # split the sentences into words\n",
    "    words = text.split() \n",
    "    for i in range(len(words)):\n",
    "        # remove words with length 1\n",
    "        if len(words[i]) == 1:\n",
    "            words[i] = ''\n",
    "        # remove repetition of letters\n",
    "        _tmp = re.subn(r'([a-z])\\1{3,}', r'\\1', words[i])\n",
    "        if _tmp[1] > 0:\n",
    "            words[i] = \"very \" + _tmp[0]\n",
    "    # remove stopwords like to, and, or etc.\n",
    "    words = [word for word in words if word not in stopwords]\n",
    "    # lemmatize each word\n",
    "    words = [wordnet_lemmatizer.lemmatize(word) for word in words]\n",
    "    # join words to make sentence\n",
    "    text = ' '.join(words)\n",
    "    # remove multiple spaces\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# drop rows with missing values\n",
    "df = df.dropna()\n",
    "df_test = df_test.dropna()\n",
    "\n",
    "# clean text data\n",
    "df['text'] = df['text'].progress_apply(clean_text)\n",
    "df.dropna()\n",
    "df.to_csv('data_cleaned.csv', index=False)    \n",
    "\n",
    "df_test['text'] = df_test['text'].progress_apply(clean_text)\n",
    "df_test.dropna()\n",
    "df_test.to_csv('data_valid_cleaned.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-28T12:29:16.738821800Z",
     "start_time": "2023-07-28T12:28:31.197070500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "' find phone charger very very switching service old phone'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test cleaning\n",
    "assert clean_text(\"LOOOOOOOOOOVE\") == \"very very love\"\n",
    "assert clean_text(\"@herrmotz\") == \"\"\n",
    "assert clean_text(\"https://youtube.com/test\") == \"\"\n",
    "\n",
    "clean_text(\"can't find my phone charger.. So I'm switching my service over to my old phone\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-28T12:22:02.681115400Z",
     "start_time": "2023-07-28T12:22:02.644338500Z"
    }
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "# read cleaned data\n",
    "df = pd.read_csv('data_cleaned.csv')\n",
    "df_test = pd.read_csv('data_valid_cleaned.csv')\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "# read raw data\n",
    "df = pd.read_csv('data.csv')\n",
    "df_test = pd.read_csv('data_valid.csv')\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "        target                                               text\n0            0  last tweet day goodnight twitter world hope be...\n1            1  ahaha okay yeah fer suree vote right thee dire...\n2            0  very gonna feel like shit uni today very still...\n3            0   find phone charger very very switching servic...\n4            0                                    very fade black\n...        ...                                                ...\n499995       1                   very watching twilight very text\n499996       1       usual day fighting little inner outer daemon\n499997       0  very question very yesterday emotional day tod...\n499998       0                                      okay made sad\n499999       0        very very sad say anything make feel better\n\n[500000 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>last tweet day goodnight twitter world hope be...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>ahaha okay yeah fer suree vote right thee dire...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>very gonna feel like shit uni today very still...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>find phone charger very very switching servic...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>very fade black</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>499995</th>\n      <td>1</td>\n      <td>very watching twilight very text</td>\n    </tr>\n    <tr>\n      <th>499996</th>\n      <td>1</td>\n      <td>usual day fighting little inner outer daemon</td>\n    </tr>\n    <tr>\n      <th>499997</th>\n      <td>0</td>\n      <td>very question very yesterday emotional day tod...</td>\n    </tr>\n    <tr>\n      <th>499998</th>\n      <td>0</td>\n      <td>okay made sad</td>\n    </tr>\n    <tr>\n      <th>499999</th>\n      <td>0</td>\n      <td>very very sad say anything make feel better</td>\n    </tr>\n  </tbody>\n</table>\n<p>500000 rows Ã— 2 columns</p>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the csv file\n",
    "df = pd.read_csv('data_cleaned.csv')\n",
    "df_test = pd.read_csv('data_valid_cleaned.csv')\n",
    "\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-28T12:29:29.577473200Z",
     "start_time": "2023-07-28T12:29:29.016497200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "# def drop_not_string(_df, column):\n",
    "#     return _df.drop(_df[_df[column].apply(lambda x: isinstance(x, str)) == False].index)\n",
    "# \n",
    "# drop_not_string(df, 'text')\n",
    "# drop_not_string(df_test, 'text')\n",
    "\n",
    "X = df['text']\n",
    "y = df['target']\n",
    "\n",
    "X_test = df_test['text']\n",
    "\n",
    "# make them all strings\n",
    "X = X.astype(str)\n",
    "X_test = X_test.astype(str)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-28T12:39:02.177869Z",
     "start_time": "2023-07-28T12:39:02.137336500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/99988 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c721797bc0224d31b9b83a61db07c584"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "(491883, 100)"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = [sentence.split() for sentence in X]\n",
    "w2v_model = Word2Vec(sentences, window=5, min_count=5, workers=4, hs=1 , negative=0)\n",
    "\n",
    "def vectorize(sentence):\n",
    "    words = sentence.split()\n",
    "    words_vecs = [w2v_model.wv[word] for word in words if word in w2v_model.wv]\n",
    "    if len(words_vecs) == 0:\n",
    "        return np.zeros(100)\n",
    "    words_vecs = np.array(words_vecs)\n",
    "    return words_vecs.mean(axis=0)\n",
    "\n",
    "X_w2v = np.array([vectorize(sentence) for sentence in tqdm(X)])\n",
    "X_test_w2v = np.array([vectorize(sentence) for sentence in tqdm(X_test)])\n",
    "\n",
    "X_w2v.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-28T12:39:19.564954300Z",
     "start_time": "2023-07-28T12:39:04.000694900Z"
    }
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "from datetime import time\n",
    "from matplotlib.pyplot import plot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "\n",
    "from gensim.models import LdaModel\n",
    "from gensim import corpora\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "light_grey_tup=(0.9, 0.9, 0.9)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "def predict(X : np.ndarray, theta: np.ndarray) -> np.ndarray:\n",
    "    '''\n",
    "    Predicts Y given X and theta.\n",
    "    \n",
    "    @Params:\n",
    "        X... matrix with datapoints as rows (m x n)\n",
    "        theta... parameter vector (n)\n",
    "        \n",
    "    @Returns:\n",
    "        array of predictions (m)\n",
    "    '''\n",
    "    # implement\n",
    "    return np.where(X @ theta >= 0, 1, 0)\n",
    "\n",
    "def sigmoid(X : np.ndarray) -> np.ndarray:\n",
    "    '''\n",
    "    Sigmoid function.\n",
    "    \n",
    "    @Params:\n",
    "        X... array of x\n",
    "        \n",
    "    @Returns:\n",
    "        array of sigmoid(x)\n",
    "    '''\n",
    "    # implement\n",
    "    return 1 / (1 + np.exp(-1 * X))\n",
    "\n",
    "def acc(X : np.ndarray, Y : np.ndarray, theta : np.ndarray) -> float:\n",
    "    '''\n",
    "    Accuracy.\n",
    "    \n",
    "    @Params:\n",
    "        X... matrix with datapoints as rows (m x n)\n",
    "        Y... array of true labels (n)\n",
    "        theta... parameter vector (n)\n",
    "        \n",
    "    @Returns:\n",
    "        binary cross entropy\n",
    "    '''\n",
    "    \n",
    "    # implement\n",
    "    labels = Y.flatten()\n",
    "    predictions = predict(X, theta).flatten()\n",
    "\n",
    "    return np.mean(list(map(lambda x_i, y_i: x_i*y_i + (1 - x_i)*(1 - y_i), labels, predictions))).__float__()\n",
    "\n",
    "def cross_entropy_loss(X : np.ndarray, Y : np.ndarray, theta : np.ndarray) -> float:\n",
    "    '''\n",
    "    Binary cross entropy loss.\n",
    "    \n",
    "    @Params:\n",
    "        X... matrix with datapoints as rows (m x n)\n",
    "        Y... array of true labels (n)\n",
    "        theta... parameter vector (n)\n",
    "        \n",
    "    @Returns:\n",
    "        binary cross entropy\n",
    "    '''\n",
    "    # implement\n",
    "\n",
    "    p = sigmoid(X @ theta)\n",
    "    return -1 * np.mean(Y * np.log(p) + (1-Y) * np.log(1-p))\n",
    "\n",
    "def cross_entropy_gradient(X : np.ndarray, Y : np.ndarray, theta : np.ndarray):\n",
    "    '''\n",
    "    Gradient of binary crossentropy. wrt theta.\n",
    "    \n",
    "    @Params:\n",
    "        X... matrix with datapoints as rows (m x n)\n",
    "        Y... array of true labels (n)\n",
    "        theta... parameter vector (n)\n",
    "        \n",
    "    @Returns:\n",
    "        gradient at point theta\n",
    "    '''\n",
    "    # implement\n",
    "    return 1/X.shape[0] * (X.T @ (sigmoid(X @ theta) - Y))\n",
    "\n",
    "class LogReg():\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Class for Logistic regression.\n",
    "        '''\n",
    "        self.theta = None # parameter vector\n",
    "        self.accs = [] # accuracies\n",
    "        self.losses = [] # losses\n",
    "\n",
    "\n",
    "    def predict(self, X : np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        Predicts Y given X and learned theta.\n",
    "\n",
    "        @Params:\n",
    "            X... matrix with datapoints as rows (m x n)\n",
    "\n",
    "        @Returns:\n",
    "            array of predictions (m)\n",
    "        '''\n",
    "        return predict(X, self.theta)\n",
    "    \n",
    "    \n",
    "    def fit(self, X : np.ndarray, Y : np.ndarray, lr : float = 1e-2, max_it : int = 1000, eps : float = 1e-5) -> None:\n",
    "        '''\n",
    "        Gradient descend for binary crossentropy.\n",
    "\n",
    "        @Params:\n",
    "            X... matrix with datapoints as rows (m x n)\n",
    "            Y... array of true labels (n)\n",
    "            lr... learnrate, sets stepsize for descend\n",
    "            max_it... maximum number of steps\n",
    "            eps... abort criterium for early stopping (loss did not change more than this)\n",
    "\n",
    "        '''\n",
    "\n",
    "        self.theta = np.random.rand(X.shape[1])\n",
    "        print(\"theta\", self.theta.shape)\n",
    "\n",
    "        for i in range(max_it):\n",
    "            loss = cross_entropy_loss(X, Y, self.theta)\n",
    "            accuracy = acc(X, Y, self.theta)\n",
    "\n",
    "            self.losses.append(loss)\n",
    "            self.accs.append(accuracy)\n",
    "\n",
    "            ceg = cross_entropy_gradient(X, Y, self.theta)\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Iter {i}, Accuracy: {accuracy}\")\n",
    "\n",
    "            if loss < eps:\n",
    "                break\n",
    "\n",
    "            self.theta -= ceg\n",
    "\n",
    "        print(\"\\nAccuracy start:\", self.accs[0], \"\\nAccuracy finish:\", self.accs[-1])\n",
    "        print(\"Finished due to\", \"max_it reached\" if i == max_it-1 else \"change smaller than epsilon\")\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "# use sklearn to learn a logistic regression classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "classifier = LogisticRegression(max_iter=1000)\n",
    "classifier.fit(X_w2v, y)\n",
    "\n",
    "# predict the test set results\n",
    "y_pred = classifier.predict(X_test_w2v)\n",
    "\n",
    "# save the results to npy\n",
    "df_test['target'] = y_pred\n",
    "np.save('y_pred.npy', y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-28T12:40:14.652991700Z",
     "start_time": "2023-07-28T12:40:11.862442700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy:  0.7310803417328062\n",
      "standard deviation:  0.0031705444646507553\n"
     ]
    }
   ],
   "source": [
    "# make k-fold cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator=classifier, X=X_w2v, y=y, cv=10)\n",
    "print('mean accuracy: ', accuracies.mean())\n",
    "print('standard deviation: ', accuracies.std())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-28T12:40:40.319124Z",
     "start_time": "2023-07-28T12:40:15.620632400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transformer\n",
    "Using a pre-trained transformer for sentiment analysis from Huggingface.\n",
    "Score on the validation set: 0.71\n",
    "\n",
    "This method is therefore worse than what I used before..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "from transformers import pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "y_pred_transformer = sentiment_pipeline(df_test['text'].tolist())\n",
    "\n",
    "y_pred_transformer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "# np.array([1 if x['label'] == 'POSITIVE' else 0 for x in y_pred_transformer]).tofile('y_pred_transformer.npy')\n",
    "y_pred_transformer_array = np.array(y_pred_transformer)\n",
    "np.save('y_pred_transformer.npy', y_pred_transformer_array)\n",
    "np.array(y_pred_transformer).shape[0] == len(df_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "# type(df['text'].tolist())\n",
    "_x = np.load(\"y_pred_transformer.npy\", allow_pickle=True)\n",
    "np.all(_x == y_pred_transformer_array)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
