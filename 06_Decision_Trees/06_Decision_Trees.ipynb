{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaacd30f",
   "metadata": {},
   "source": [
    "# Exercise 6 - Decision Trees\n",
    "\n",
    "In this exercise we will implement a decision tree classifier and evaluate its performance.\n",
    "\n",
    "In the event of a persistent problem, do not hesitate to contact the course instructor under\n",
    "\n",
    "- paul.kahlmeyer@uni-jena.de\n",
    "\n",
    "### Submission\n",
    "- Deadline of submission:\n",
    "        24.05.23 23:59\n",
    "- Submission on [moodle page](https://moodle.uni-jena.de/course/view.php?id=43681)\n",
    "\n",
    "\n",
    "### Help\n",
    "In case you cannot solve a task, you can use the saved values within the `help` directory:\n",
    "- Load arrays with [Numpy](https://numpy.org/doc/stable/reference/generated/numpy.load.html)\n",
    "```\n",
    "np.load('help/array_name.npy')\n",
    "```\n",
    "- Load functions with [Dill](https://dill.readthedocs.io/en/latest/dill.html)\n",
    "```\n",
    "import dill\n",
    "with open('help/some_func.pkl', 'rb') as f:\n",
    "    func = dill.load(f)\n",
    "```\n",
    "\n",
    "to continue working on the other tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "2aff8a5f",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Here we will use a synthetic dataset for binary classification created with scikit learn (see [here](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ddaa33",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "Load and display the data located in `X.npy` and `y.npy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0055264d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T17:35:09.123801263Z",
     "start_time": "2023-05-22T17:35:09.077946210Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 1.33685399, -1.18802063,  1.        ],\n       [-0.86860457,  0.96199178,  1.        ],\n       [ 1.01438883,  1.11659789,  1.        ],\n       ...,\n       [-0.69498909,  0.82369382,  0.        ],\n       [-1.43029196,  1.0429992 ,  0.        ],\n       [-0.59294741,  0.967871  ,  0.        ]])"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: load and display data\n",
    "import numpy as np\n",
    "\n",
    "X = np.load(\"X.npy\")\n",
    "y = np.load(\"y.npy\")\n",
    "\n",
    "\n",
    "np.column_stack((X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6643a5fb",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "Decision trees are data structures, where at each level we make a decision about a feature and at the lowest level we have a decision for a class. For example, consider the following decision tree:\n",
    "<div>\n",
    "<img src=\"images/decision_tree.png\" width=\"800\"/>\n",
    "</div>\n",
    "You can think of a datapoint trickling down starting at the root and ending up in one of the leaf nodes that assign a class label.\n",
    "\n",
    "That is, learning a decision tree involves learning which question to ask at which node.\n",
    "\n",
    "## Learning\n",
    "\n",
    "Usually we want to select questions in a way that \"unmix\" the labels of a dataset the best.\n",
    "\n",
    "As an example, consider a dataset like this:\n",
    "\n",
    "`X = [1, 2, 3, 4]`, `y = [0, 0, 1, 1]`\n",
    "\n",
    "Here, we could ask the question: \"$X\\leq 1$?\", which would split the dataset into \n",
    "\n",
    "`X1 = [1]`, `y1 = [0]` and \n",
    "\n",
    "`X2 = [2, 3, 4]`, `y2 = [0, 1, 1]`.\n",
    "\n",
    "looking at the labels of the resulting datasets `y1` and `y2`, we can see that `y1` is nicely unmixed (contains only labels of one class). However `y2` is not unmixed.\n",
    "\n",
    "A better question would be to ask: \"$X\\leq 2$?\", in which case we would get\n",
    "\n",
    "`X1 = [1, 2]`, `y1 = [0, 0]` and \n",
    "\n",
    "`X2 = [3, 4]`, `y2 = [1, 1]`.\n",
    "\n",
    "Here both `y1` and `y2` are perfectly unmixed.\n",
    "\n",
    "There are many ways to quantify the \"unmixing\" property of a dataset. For decision trees however a popular choice is the [**Gini-Impurity**](https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity). It measures the probability that we would missclasify a sample if we classified according to the distribution of labels.\n",
    "\n",
    "Let $p_i$ be the relative frequency of class $i$ in the dataset with labels $y$, then the Gini-Impurity is defined as\n",
    "\n",
    "\\begin{align*}\n",
    "I_G(y) = 1 - \\sum_i p_i^2\\,.\n",
    "\\end{align*}\n",
    "\n",
    "In the following figure we can see the Gini-Impurity for binary classification problems depending on the relative frequency $p_0$ of class 0: \n",
    "<div>\n",
    "<img src=\"images/gini.png\" width=\"500\"/>\n",
    "</div>\n",
    "If we have a perfectly unmixed dataset, the Gini-Impurity is 0. The maximum value is 0.5 if we have the same amount of both labels. Recall that for decision trees we want to split a dataset into subsets of low Gini-Impurity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecd9a33",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Implement the following function and calculate the Gini-Impurity for the labels of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7fbd0252",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T17:22:37.470212026Z",
     "start_time": "2023-05-22T17:22:37.428294302Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "('Gini-Impurity', 0.500072)"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gini_imp(y : np.ndarray) -> float:\n",
    "    '''\n",
    "    Calculates Gini-Impurity for binary labels.\n",
    "    \n",
    "    @Params:\n",
    "        y... labels in {0, 1}\n",
    "        \n",
    "    @Returns: \n",
    "        Gini-Impurity\n",
    "    '''\n",
    "    # TODO: implement\n",
    "    p0 = np.mean(y)\n",
    "    p1 = 1-p0\n",
    "\n",
    "    return p0**2 + p1**2\n",
    "\n",
    "# TODO: Gini-Impurity on dataset\n",
    "'Gini-Impurity', gini_imp(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06efc7a",
   "metadata": {},
   "source": [
    "We want to use the Gini-Impurity to evaluate how good a splitting of a dataset is. For this we simply calculate a weighted sum of the impurities of the individual splits. Let $y_0, y_1$ be splits of a dataset $y$. Then we simply compute the score\n",
    "\n",
    "\\begin{align*}\n",
    "S(y_0, y_1) = \\cfrac{|y_0|}{|y_0| + |y_1|}I_G(y_0) + \\cfrac{|y_1|}{|y_0| + |y_1|}I_G(y_1)\\,.\n",
    "\\end{align*}\n",
    "\n",
    "That is we weight each the individual impurities with the size of the splits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f71e8f4",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "Implement the following function and calculate the score if we would split the dataset according to the question \"$x_0 \\leq 0$?\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ce6f1ba6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T17:25:48.844145876Z",
     "start_time": "2023-05-22T17:25:48.806054053Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.7807446538391782"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gini_imp_split(y0 : np.ndarray, y1 : np.ndarray) -> float:\n",
    "    '''\n",
    "    Weighted Gini-Impurity for a split into two sets of labels (binary classification).\n",
    "    \n",
    "    @Params:\n",
    "        y0... label set\n",
    "        y1... label set\n",
    "    \n",
    "    @Returns:\n",
    "        Impurity score (lower = better)\n",
    "    '''\n",
    "    # TODO: implement\n",
    "    impurity0 = gini_imp(y0)\n",
    "    impurity1 = gini_imp(y1)\n",
    "\n",
    "    return len(y0)/len(y)*impurity0 + len(y1)/len(y)*impurity1\n",
    "\n",
    "# TODO: calculate score for split x0 <= 0\n",
    "mask = (X[:, 0] <= 0)\n",
    "gini_imp_split(y[mask], y[~mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97461ac6",
   "metadata": {},
   "source": [
    "Now the challenge is to find the index and the value that produces the lowest score ($\\rightarrow$ best split).\n",
    "For decision trees we do this in three steps:\n",
    "\n",
    "1. Iterate trough all $x\\in X$ and all dimensions $i$. Let $x_i = v$ be the value that we observe.\n",
    "2. Score the split according to \"$x_i \\leq v$?\".\n",
    "3. Report dimension $i$ and value $v$ of best score\n",
    "\n",
    "In practice we do not use the value $v$ but rather the average of $v$ and the first value for $x_i$ that is $> v$. \n",
    "\n",
    "As an example, if we identified $x_0 \\leq 1.5$ as the best split, we will look for the lowest value of $x_0$ that is $>1.5$ and report the mean of those two values. Lets say this next greater value is $1.6$, then we will report index 0 and value $0.5(1.5 + 1.6) = 1.55$. This way we draw the decision boundary between the datapoints rather than on the datapoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1f9c82",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "\n",
    "Implement the following function that searches for the best split of a dataset.\n",
    "\n",
    "Use this function to identify the best split of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "50da74de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T17:33:20.929603095Z",
     "start_time": "2023-05-22T17:33:20.833788005Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(0, -0.17473169364175, 0.19709883984704135)"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def best_split(X : np.ndarray, y : np.ndarray) -> tuple:\n",
    "    '''\n",
    "    Identifies the best split of a dataset according to Gini-Impurity.\n",
    "    \n",
    "    @Params:\n",
    "        X... samples\n",
    "        y... labels\n",
    "        \n",
    "    @Returns:\n",
    "        triple (index, value, Gini-Score) of split\n",
    "    '''\n",
    "    # TODO: implement\n",
    "\n",
    "    ret_w_imp = 1000 # just so that premature termination is visually clear.\n",
    "    ret_i = -1\n",
    "    ret_v = -1\n",
    "\n",
    "    for x in X:\n",
    "        for split_idx in range(len(x)):\n",
    "            split_v = x[split_idx]\n",
    "\n",
    "            mask = (X[:, split_idx] <= split_v)\n",
    "            y1 = y[mask]\n",
    "            y2 = y[~mask]\n",
    "\n",
    "            if len(y1) > 0 and len(y2) > 0:\n",
    "                # impurity for y1\n",
    "                p0 = np.mean(y1)\n",
    "                p1 = 1 - p0\n",
    "                imp1 = 1 - (p0**2 + p1**2)\n",
    "\n",
    "                # impurity for y2\n",
    "                p0 = np.mean(y2)\n",
    "                p1 = 1 - p0\n",
    "                imp2 = 1 - (p0**2 + p1**2)\n",
    "\n",
    "                w_imp = len(y1)/len(y)*imp1 + len(y2)/len(y)*imp2\n",
    "                if w_imp < ret_w_imp:\n",
    "                    ret_w_imp = w_imp\n",
    "                    ret_v = split_v\n",
    "                    ret_i = split_idx\n",
    "\n",
    "    return (ret_i, ret_v, ret_w_imp)\n",
    "\n",
    "# TODO: find best split for dataset\n",
    "\n",
    "best_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a599f2c1",
   "metadata": {},
   "source": [
    "With this functionality we have everything to recursively build up a decision tree.\n",
    "\n",
    "Each node in the tree has the following attributes:\n",
    "\n",
    "- `X`, `y`, `gi`: part of the dataset that is passed to the node\n",
    "- `depth`, `max_depth`: depth of the node in the tree as well as the maximum depth of the tree\n",
    "\n",
    "And can perform the following operations:\n",
    "\n",
    "- `split`: if not already pure or at `max_depth`, finds the best split of its dataset and creates two child nodes (+1 depth)\n",
    "- `predict` : given a dataset predicts labels accoding to its children (or according to its labels if we have no children)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17e36b8",
   "metadata": {},
   "source": [
    "### Task 5\n",
    "\n",
    "Implement the following `Node` class which implements the functionality of a single node. \n",
    "\n",
    "Use the provided `DecisionTree` class to learn and print a decision tree on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7b035704",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T17:45:04.772702579Z",
     "start_time": "2023-05-22T17:45:04.729793653Z"
    }
   },
   "outputs": [],
   "source": [
    "class Node():\n",
    "        \n",
    "    def __init__(self, X : np.ndarray, y : np.ndarray, depth : int, max_depth : int):\n",
    "        '''\n",
    "        Constructor for a node.\n",
    "        \n",
    "        @Params:\n",
    "            X... samples\n",
    "            y... labels\n",
    "            depth... depth of this node in the tree\n",
    "            max_depth... maximum depth of any node in the tree\n",
    "        '''\n",
    "        # TODO: implement\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.depth = depth\n",
    "        self.max_depth = max_depth\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        pass\n",
    "        \n",
    "        \n",
    "        \n",
    "    def split(self):\n",
    "        '''\n",
    "        Creates child nodes if labels are not already pure.\n",
    "        '''\n",
    "        # TODO: implement\n",
    "        if len(X) < 2:\n",
    "            return\n",
    "        else:\n",
    "            i, v, g = best_split(X, y)\n",
    "            mask = (X[:, i] <= v)\n",
    "            y1 = y[mask]\n",
    "            y2 = y[~mask]\n",
    "            self.left = Node(X[mask], y1, self.depth+1, self.max_depth)\n",
    "            self.right = Node(X[~mask], y2, self.depth+1, self.max_depth)\n",
    "\n",
    "        \n",
    "    def predict(self, X : np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        Predicts labels for samples.\n",
    "        \n",
    "        @Params:\n",
    "            X... samples\n",
    "            \n",
    "        @Returns:\n",
    "            predicted labels by this node\n",
    "        '''\n",
    "        # TODO: implement\n",
    "        pass\n",
    "            \n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        '''\n",
    "        String representation of the node.\n",
    "        \n",
    "        @Returns:\n",
    "            a string that represents the subtree with this node as root.\n",
    "            Something like \n",
    "            x0 < 1.5?\n",
    "                str(child1)\n",
    "            else\n",
    "                str(child2)\n",
    "            \n",
    "        '''\n",
    "        # TODO: implement\n",
    "        pass\n",
    "            \n",
    "class DecisionTree():\n",
    "    '''\n",
    "    Provided. A wrapper class for the root of the decision tree.\n",
    "    This will give you an idea of how the Node class is used.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, max_depth : int = np.inf):\n",
    "        '''\n",
    "        Constructor of decision tree.\n",
    "        \n",
    "        @Params:\n",
    "            max_depth... maximum depth of any node in the tree\n",
    "        '''\n",
    "        self.root = None\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    def fit(self, X : np.ndarray, y : np.ndarray):\n",
    "        '''\n",
    "        Learns the nodes of the decision tree.\n",
    "        \n",
    "        @Params:\n",
    "            X... samples\n",
    "            y... labels\n",
    "        '''\n",
    "        self.root = Node(X, y, 0, self.max_depth)\n",
    "        self.root.split()\n",
    "        \n",
    "    def predict(self, X : np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        Predicts labels for samples.\n",
    "        \n",
    "        @Params:\n",
    "            X... samples\n",
    "        '''\n",
    "        return self.root.predict(X)\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        '''\n",
    "        String representation of the tree.\n",
    "        '''\n",
    "        return str(self.root)\n",
    "    \n",
    "# TODO: learn + print decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1a9cb7",
   "metadata": {},
   "source": [
    "### Task 6\n",
    "\n",
    "Calculate the accuracy of the decision tree on the dataset and visualize the decision boundary.\n",
    "\n",
    "You can use `utils.plot_dec_boundary` for the visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222c897d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: accuracy and decision boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a61294",
   "metadata": {},
   "source": [
    "### Task 7\n",
    "\n",
    "Use the [scikit learn implementation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) to learn a decision tree with the same maximum depth as your tree from Task 5.\n",
    "\n",
    "Compare the decision boundary, accuracy and the internal parameters to those of your own implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c72115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use scikit learn implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b62116a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compare decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e3591a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compare accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c942572",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO: compare parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a783c57",
   "metadata": {},
   "source": [
    "## Influence of Tree depth\n",
    "\n",
    "Now we want to investigate the influence of the maximum tree depth on the classification accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb032fb",
   "metadata": {},
   "source": [
    "### Task 8\n",
    "\n",
    "Split the dataset into train- and testdata and plot the train- and test accuracy for an increasing maximum tree depth.\n",
    "\n",
    "You can use the scikit learn implementation or your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11141a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot train and test accuracy for increasing tree depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512f2824",
   "metadata": {},
   "source": [
    "### Task 9\n",
    "\n",
    "In your own words, describe the influence of the maximum tree depth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9ec0f2",
   "metadata": {},
   "source": [
    "<font color='red'>TODO:</font> write"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b39a671858085d70125f4f28cb6256d6037eab582b2bde2ee45c6eac63f37da5"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
